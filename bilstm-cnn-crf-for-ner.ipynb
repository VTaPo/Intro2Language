{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-15T10:30:36.593464Z","iopub.status.busy":"2024-01-15T10:30:36.593149Z","iopub.status.idle":"2024-01-15T10:30:53.604576Z","shell.execute_reply":"2024-01-15T10:30:53.603462Z","shell.execute_reply.started":"2024-01-15T10:30:36.593435Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.24.3)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=8a6fb7ddf74a40e1ae773e9b9f72c8899823161f46ff4a393637d8a44546f5c0\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n"]}],"source":["!pip install seqeval"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:30:53.607006Z","iopub.status.busy":"2024-01-15T10:30:53.606657Z","iopub.status.idle":"2024-01-15T10:30:59.860396Z","shell.execute_reply":"2024-01-15T10:30:59.859369Z","shell.execute_reply.started":"2024-01-15T10:30:53.606980Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_42/244750557.py:16: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n","  plt.style.use('seaborn-pastel')\n","/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"name":"stdout","output_type":"stream","text":["/kaggle/input/covid19vi/word/dev_word.conll\n","/kaggle/input/covid19vi/word/train_word.conll\n","/kaggle/input/covid19vi/word/test_word.json\n","/kaggle/input/covid19vi/word/dev_word.json\n","/kaggle/input/covid19vi/word/test_word.conll\n","/kaggle/input/covid19vi/word/train_word.json\n","/kaggle/input/covid19vi/syllable/test_syllable.json\n","/kaggle/input/covid19vi/syllable/dev_syllable.json\n","/kaggle/input/covid19vi/syllable/test_syllable.conll\n","/kaggle/input/covid19vi/syllable/train_syllable.conll\n","/kaggle/input/covid19vi/syllable/dev_syllable.conll\n","/kaggle/input/covid19vi/syllable/train_syllable.json\n","/kaggle/input/word2vec-vi-words-100dims/word2vec_vi_words_100dims.txt\n","/kaggle/input/word2vec-vi-syllables-100dims/word2vec_vi_syllables_100dims.txt\n"]}],"source":["from __future__ import print_function\n","from collections import OrderedDict\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import init\n","from torch.autograd import Variable\n","from torch import autograd\n","\n","import time\n","import _pickle as cPickle\n","\n","import urllib\n","import matplotlib.pyplot as plt\n","plt.rcParams['figure.dpi'] = 80\n","plt.style.use('seaborn-pastel')\n","\n","import os\n","import sys\n","import codecs\n","import re\n","import numpy as np\n","\n","#import các thư viện cần thiết\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","#Embedding\n","from transformers import BertTokenizerFast, BertForTokenClassification, BertModel\n","\n","#Modeling\n","from torch.utils.data import DataLoader\n","from torch.optim import SGD, Adam\n","from seqeval.metrics import classification_report\n","from tqdm import tqdm\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:30:59.862421Z","iopub.status.busy":"2024-01-15T10:30:59.861832Z","iopub.status.idle":"2024-01-15T10:30:59.918325Z","shell.execute_reply":"2024-01-15T10:30:59.917107Z","shell.execute_reply.started":"2024-01-15T10:30:59.862383Z"},"trusted":true},"outputs":[],"source":["#parameters for the Model\n","parameters = OrderedDict()\n","parameters['train'] = \"/kaggle/input/covid19vi/syllable/train_syllable.conll\" #Path to train file\n","parameters['dev'] = \"/kaggle/input/covid19vi/syllable/dev_syllable.conll\" #Path to test file\n","parameters['test'] = \"/kaggle/input/covid19vi/syllable/test_syllable.conll\" #Path to dev file\n","#parameters['tag_scheme'] = \"BIOES\" #BIO or BIOES\n","parameters['lower'] = True # Boolean variable to control lowercasing of words\n","parameters['zeros'] =  True # Boolean variable to control replacement of  all digits by 0 \n","parameters['char_dim'] = 50 #Char embedding dimension\n","parameters['word_dim'] = 100 #Token embedding dimension\n","parameters['word_lstm_dim'] = 200 #Token LSTM hidden layer size\n","parameters['word_bidirect'] = True #Use a bidirectional LSTM for words\n","parameters['embedding_path'] = \"/kaggle/input/word2vec-vi-syllables-100dims/word2vec_vi_syllables_100dims.txt\" #Location of pretrained embeddings\n","parameters['all_emb'] = 1 #Load all embeddings\n","parameters['crf'] =1 #Use CRF (0 to disable)\n","parameters['dropout'] = 0.25 #Droupout on the input (0 = no dropout)\n","parameters['epoch'] =  30 #Number of epochs to run\"\n","parameters['weights'] = \"\" #path to Pretrained for from a previous run\n","parameters['name'] = \"BCC_syllable\" # Model name\n","#parameters['gradient_clip']=5.0\n","parameters['char_mode']=\"CNN\"\n","models_path = \"./models/\" #path to saved models\n","\n","#GPU\n","parameters['use_gpu'] = torch.cuda.is_available() #GPU Check\n","use_gpu = parameters['use_gpu']\n","\n","#parameters['reload'] = \"/kaggle/input/named-entity-recognition/pre-trained-model\" \n","\n","#Constants\n","START_TAG = '<START>'\n","STOP_TAG = '<STOP>'"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:30:59.921174Z","iopub.status.busy":"2024-01-15T10:30:59.920812Z","iopub.status.idle":"2024-01-15T10:30:59.934499Z","shell.execute_reply":"2024-01-15T10:30:59.933699Z","shell.execute_reply.started":"2024-01-15T10:30:59.921131Z"},"trusted":true},"outputs":[],"source":["#paths to files \n","# #To stored mapping file\n","# mapping_file = '/kaggle/input/named-entity-recognition/mapping.pkl'\n","\n","#To stored model\n","name = parameters['name']\n","model_name = models_path + name #get_name(parameters)\n","\n","if not os.path.exists(models_path):\n","    os.makedirs(models_path)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:30:59.935842Z","iopub.status.busy":"2024-01-15T10:30:59.935520Z","iopub.status.idle":"2024-01-15T10:30:59.951871Z","shell.execute_reply":"2024-01-15T10:30:59.950989Z","shell.execute_reply.started":"2024-01-15T10:30:59.935815Z"},"trusted":true},"outputs":[],"source":["#hàm đọc dữ liệu\n","def read_dataset(file_path):\n","    tokens=[]\n","    ner_tags=[]\n","    ids=[]\n","    count=1\n","    with open(file_path) as f:\n","        lines=f.readlines()\n","        ts=[]\n","        nts=[]\n","        for line in lines:\n","            line = line.split()\n","            if len(line)==0:\n","                ids.append(count)\n","                tokens.append(ts)\n","                ner_tags.append(nts)\n","                ts=[]\n","                nts=[]\n","                count+=1\n","            else:\n","                ts.append(line[0])\n","                nts.append(line[-1])\n","    data = pd.DataFrame({'Id':ids, 'NER_tags':ner_tags, 'Tokens':tokens})\n","    return data"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:30:59.953343Z","iopub.status.busy":"2024-01-15T10:30:59.953005Z","iopub.status.idle":"2024-01-15T10:30:59.965908Z","shell.execute_reply":"2024-01-15T10:30:59.964989Z","shell.execute_reply.started":"2024-01-15T10:30:59.953311Z"},"trusted":true},"outputs":[],"source":["def get_NER_labels(data):\n","    NERs = list(data['NER_tags'].values)\n","    labels_list = []\n","    for value in NERs:\n","        labels_list = labels_list + value\n","    types = list(set(labels_list))\n","    return types"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:30:59.967401Z","iopub.status.busy":"2024-01-15T10:30:59.967088Z","iopub.status.idle":"2024-01-15T10:31:00.375541Z","shell.execute_reply":"2024-01-15T10:31:00.374535Z","shell.execute_reply.started":"2024-01-15T10:30:59.967369Z"},"trusted":true},"outputs":[],"source":["train_df = read_dataset('/kaggle/input/covid19vi/syllable/train_syllable.conll')\n","val_df = read_dataset('/kaggle/input/covid19vi/syllable/dev_syllable.conll')\n","test_df = read_dataset('/kaggle/input/covid19vi/syllable/test_syllable.conll')"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:31:00.377519Z","iopub.status.busy":"2024-01-15T10:31:00.376870Z","iopub.status.idle":"2024-01-15T10:31:00.424615Z","shell.execute_reply":"2024-01-15T10:31:00.423361Z","shell.execute_reply.started":"2024-01-15T10:31:00.377483Z"},"trusted":true},"outputs":[],"source":["words_list = train_df['Tokens'].values.tolist()+val_df['Tokens'].values.tolist()+test_df['Tokens'].values.tolist()\n","words = set([t for l in words_list for t in l])"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:31:00.426337Z","iopub.status.busy":"2024-01-15T10:31:00.426034Z","iopub.status.idle":"2024-01-15T10:31:00.438424Z","shell.execute_reply":"2024-01-15T10:31:00.437448Z","shell.execute_reply.started":"2024-01-15T10:31:00.426311Z"},"trusted":true},"outputs":[],"source":["chars = set([c for word in words for c in word])"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:31:00.443553Z","iopub.status.busy":"2024-01-15T10:31:00.443252Z","iopub.status.idle":"2024-01-15T10:31:02.799180Z","shell.execute_reply":"2024-01-15T10:31:02.798403Z","shell.execute_reply.started":"2024-01-15T10:31:00.443523Z"},"trusted":true},"outputs":[],"source":["unique_labels = get_NER_labels(train_df)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:31:02.800773Z","iopub.status.busy":"2024-01-15T10:31:02.800379Z","iopub.status.idle":"2024-01-15T10:31:02.808722Z","shell.execute_reply":"2024-01-15T10:31:02.807922Z","shell.execute_reply.started":"2024-01-15T10:31:02.800724Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['O',\n"," 'B-ORGANIZATION',\n"," 'B-DATE',\n"," 'B-JOB',\n"," 'I-TRANSPORTATION',\n"," 'B-SYMPTOM_AND_DISEASE',\n"," 'B-LOCATION',\n"," 'I-DATE',\n"," 'B-GENDER',\n"," 'I-SYMPTOM_AND_DISEASE',\n"," 'I-AGE',\n"," 'B-TRANSPORTATION',\n"," 'I-LOCATION',\n"," 'I-PATIENT_ID',\n"," 'B-NAME',\n"," 'I-GENDER',\n"," 'I-ORGANIZATION',\n"," 'B-PATIENT_ID',\n"," 'I-JOB',\n"," 'B-AGE',\n"," 'I-NAME']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["unique_labels"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:31:02.810338Z","iopub.status.busy":"2024-01-15T10:31:02.809985Z","iopub.status.idle":"2024-01-15T10:31:02.823166Z","shell.execute_reply":"2024-01-15T10:31:02.822237Z","shell.execute_reply.started":"2024-01-15T10:31:02.810304Z"},"trusted":true},"outputs":[],"source":["def create_dico(words):\n","    \"\"\"\n","    Create a dictionary of items from a list of list of items.\n","    \"\"\"\n","    dico = {}\n","    for word in words:\n","        if word not in dico:\n","            dico[word] = 1\n","        else:\n","            dico[word] += 1\n","    return dico\n","\n","def create_mapping(dico):\n","    \"\"\"\n","    Create a mapping (item to ID / ID to item) from a dictionary.\n","    Items are ordered by decreasing frequency.\n","    \"\"\"\n","    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n","    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n","    item_to_id = {v: k for k, v in id_to_item.items()}\n","    return item_to_id, id_to_item\n","\n","def word_mapping(words):\n","    \"\"\"\n","    Create a dictionary and a mapping of words, sorted by frequency.\n","    \"\"\"\n","    dico = create_dico(words)\n","    dico['<UNK>'] = 10000000 #UNK tag for unknown words\n","    word_to_id, id_to_word = create_mapping(dico)\n","    print(\"Found %i unique words\" % len(dico))\n","    return dico, word_to_id, id_to_word\n","\n","def char_mapping(chars):\n","    \"\"\"\n","    Create a dictionary and mapping of characters, sorted by frequency.\n","    \"\"\"\n","    dico = create_dico(chars)\n","    char_to_id, id_to_char = create_mapping(dico)\n","    print(\"Found %i unique characters\" % len(dico))\n","    return dico, char_to_id, id_to_char\n","\n","def tag_mapping(unique_labels):\n","    \"\"\"\n","    Create a dictionary and a mapping of tags, sorted by frequency.\n","    \"\"\"\n","    dico = create_dico(unique_labels)\n","    dico[START_TAG] = -1\n","    dico[STOP_TAG] = -2\n","    tag_to_id, id_to_tag = create_mapping(dico)\n","    print(\"Found %i unique named entity tags\" % len(dico))\n","    return dico, tag_to_id, id_to_tag"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:31:02.824708Z","iopub.status.busy":"2024-01-15T10:31:02.824432Z","iopub.status.idle":"2024-01-15T10:31:02.851628Z","shell.execute_reply":"2024-01-15T10:31:02.850806Z","shell.execute_reply.started":"2024-01-15T10:31:02.824683Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 5728 unique words\n","Found 180 unique characters\n","Found 23 unique named entity tags\n"]}],"source":["dico_words,word_to_id,id_to_word = word_mapping(words)\n","dico_chars, char_to_id, id_to_char = char_mapping(chars)\n","dico_tags, tag_to_id, id_to_tag = tag_mapping(unique_labels)"]},{"cell_type":"markdown","metadata":{},"source":["# Chuẩn bị dữ liệu"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:31:02.852911Z","iopub.status.busy":"2024-01-15T10:31:02.852640Z","iopub.status.idle":"2024-01-15T10:31:03.970598Z","shell.execute_reply":"2024-01-15T10:31:03.969655Z","shell.execute_reply.started":"2024-01-15T10:31:02.852888Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["5027 / 2000 / 3000 sentences in train / dev / test.\n"]}],"source":["def prepare_dataset(sentences, labels, word_to_id, char_to_id, tag_to_id):\n","    \"\"\"\n","    Prepare the dataset. Return a list of lists of dictionaries containing:\n","        - word indexes\n","        - word char indexes\n","        - tag indexes\n","    \"\"\"\n","    data = []\n","    for s, label in zip(sentences,labels):\n","        str_words = s\n","        words = [word_to_id[w if w in word_to_id else '<UNK>'] for w in str_words]\n","        chars = [[char_to_id[c] for c in w if c in char_to_id]\n","                 for w in str_words]\n","        tags = [tag_to_id[l] for l in label]\n","        data.append({\n","            'str_words': str_words,\n","            'words': words,\n","            'chars': chars,\n","            'tags': tags,\n","        })\n","    return data\n","\n","train_data = prepare_dataset(\n","    train_df['Tokens'].values.tolist(), train_df['NER_tags'].values.tolist(), word_to_id, char_to_id, tag_to_id\n",")\n","dev_data = prepare_dataset(\n","    val_df['Tokens'].values.tolist(), val_df['NER_tags'].values.tolist(), word_to_id, char_to_id, tag_to_id\n",")\n","test_data = prepare_dataset(\n","    test_df['Tokens'].values.tolist(), test_df['NER_tags'].values.tolist(), word_to_id, char_to_id, tag_to_id\n",")\n","print(\"{} / {} / {} sentences in train / dev / test.\".format(len(train_data), len(dev_data), len(test_data)))"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:31:03.972357Z","iopub.status.busy":"2024-01-15T10:31:03.971965Z","iopub.status.idle":"2024-01-15T10:31:03.984127Z","shell.execute_reply":"2024-01-15T10:31:03.983161Z","shell.execute_reply.started":"2024-01-15T10:31:03.972322Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'str_words': ['Đồng',\n","  'thời',\n","  ',',\n","  'bệnh',\n","  'viện',\n","  'tiếp',\n","  'tục',\n","  'thực',\n","  'hiện',\n","  'các',\n","  'biện',\n","  'pháp',\n","  'phòng',\n","  'chống',\n","  'dịch',\n","  'bệnh',\n","  'COVID',\n","  '-',\n","  '19',\n","  'theo',\n","  'hướng',\n","  'dẫn',\n","  'của',\n","  'Bộ',\n","  'Y',\n","  'tế',\n","  '.'],\n"," 'words': [5538,\n","  5014,\n","  11,\n","  3607,\n","  5283,\n","  5038,\n","  5244,\n","  5022,\n","  4044,\n","  3780,\n","  3532,\n","  4684,\n","  4693,\n","  3735,\n","  3915,\n","  3607,\n","  1818,\n","  12,\n","  392,\n","  4927,\n","  4098,\n","  3908,\n","  3845,\n","  1795,\n","  3495,\n","  5221,\n","  13],\n"," 'chars': [[103, 148, 64, 57],\n","  [70, 58, 155, 59],\n","  [8],\n","  [52, 142, 64, 58],\n","  [72, 59, 142, 64],\n","  [70, 59, 136, 66],\n","  [70, 160, 53],\n","  [70, 58, 167, 53],\n","  [58, 59, 142, 64],\n","  [53, 86, 53],\n","  [52, 59, 142, 64],\n","  [66, 58, 86, 66],\n","  [66, 58, 94, 64, 57],\n","  [53, 58, 147, 64, 57],\n","  [54, 144, 53, 58],\n","  [52, 142, 64, 58],\n","  [27, 39, 46, 33, 28],\n","  [9],\n","  [13, 21],\n","  [70, 58, 55, 65],\n","  [58, 110, 154, 64, 57],\n","  [54, 126, 64],\n","  [53, 161, 51],\n","  [26, 152],\n","  [49],\n","  [70, 136],\n","  [10]],\n"," 'tags': [20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  20,\n","  6,\n","  16,\n","  16,\n","  20]}"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["train_data[0]"]},{"cell_type":"markdown","metadata":{},"source":["# Load word embeddings"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:31:03.985597Z","iopub.status.busy":"2024-01-15T10:31:03.985279Z","iopub.status.idle":"2024-01-15T10:32:16.303782Z","shell.execute_reply":"2024-01-15T10:32:16.302906Z","shell.execute_reply.started":"2024-01-15T10:31:03.985570Z"},"trusted":true},"outputs":[],"source":["all_word_embeds = {}\n","for i, line in enumerate(codecs.open(parameters['embedding_path'], 'r', 'utf-8')):\n","    s = line.strip().split()\n","    if len(s) == parameters['word_dim'] + 1:\n","        all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:32:16.305714Z","iopub.status.busy":"2024-01-15T10:32:16.305029Z","iopub.status.idle":"2024-01-15T10:32:16.333550Z","shell.execute_reply":"2024-01-15T10:32:16.332680Z","shell.execute_reply.started":"2024-01-15T10:32:16.305673Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded 977025 pretrained embeddings.\n"]}],"source":["#Intializing Word Embedding Matrix\n","word_embeds = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), parameters['word_dim']))\n","\n","for w in word_to_id:\n","    if w in all_word_embeds:\n","        word_embeds[word_to_id[w]] = all_word_embeds[w]\n","    elif w.lower() in all_word_embeds:\n","        word_embeds[word_to_id[w]] = all_word_embeds[w.lower()]\n","\n","print('Loaded %i pretrained embeddings.' % len(all_word_embeds))"]},{"cell_type":"markdown","metadata":{},"source":["# Mô hình hóa"]},{"cell_type":"markdown","metadata":{},"source":["**Viết hàm khởi tạo embeddings cho char level**"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:32:16.335588Z","iopub.status.busy":"2024-01-15T10:32:16.335307Z","iopub.status.idle":"2024-01-15T10:32:16.340317Z","shell.execute_reply":"2024-01-15T10:32:16.339269Z","shell.execute_reply.started":"2024-01-15T10:32:16.335563Z"},"trusted":true},"outputs":[],"source":["def init_embedding(input_embedding):\n","    \"\"\"\n","    Initialize embedding\n","    \"\"\"\n","    bias = np.sqrt(3.0 / input_embedding.size(1))\n","    nn.init.uniform(input_embedding, -bias, bias)"]},{"cell_type":"markdown","metadata":{},"source":["**Viết hàm khởi tạo trọng số cho ma trận trong lớp linear layer khi dự đoán đầu ra**"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:32:16.341708Z","iopub.status.busy":"2024-01-15T10:32:16.341435Z","iopub.status.idle":"2024-01-15T10:32:16.353838Z","shell.execute_reply":"2024-01-15T10:32:16.352887Z","shell.execute_reply.started":"2024-01-15T10:32:16.341684Z"},"trusted":true},"outputs":[],"source":["def init_linear(input_linear):\n","    \"\"\"\n","    Initialize linear transformation\n","    \"\"\"\n","    bias = np.sqrt(6.0 / (input_linear.weight.size(0) + input_linear.weight.size(1)))\n","    nn.init.uniform(input_linear.weight, -bias, bias)\n","    if input_linear.bias is not None:\n","        input_linear.bias.data.zero_()"]},{"cell_type":"markdown","metadata":{},"source":["**Viết hàm khởi tạo trọng số trong các lớp LSTM**"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:32:16.355263Z","iopub.status.busy":"2024-01-15T10:32:16.355007Z","iopub.status.idle":"2024-01-15T10:32:16.370612Z","shell.execute_reply":"2024-01-15T10:32:16.369689Z","shell.execute_reply.started":"2024-01-15T10:32:16.355240Z"},"trusted":true},"outputs":[],"source":["def init_lstm(input_lstm):\n","    \"\"\"\n","    Initialize lstm\n","    \n","    PyTorch weights parameters:\n","    \n","        weight_ih_l[k]: the learnable input-hidden weights of the k-th layer,\n","            of shape `(hidden_size * input_size)` for `k = 0`. Otherwise, the shape is\n","            `(hidden_size * hidden_size)`\n","            \n","        weight_hh_l[k]: the learnable hidden-hidden weights of the k-th layer,\n","            of shape `(hidden_size * hidden_size)`            \n","    \"\"\"\n","    \n","    # Weights init for forward layer\n","    for ind in range(0, input_lstm.num_layers):\n","        \n","        ## Gets the weights Tensor from our model, for the input-hidden weights in our current layer\n","        weight = eval('input_lstm.weight_ih_l' + str(ind))\n","        \n","        # Initialize the sampling range\n","        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n","        \n","        # Randomly sample from our samping range using uniform distribution and apply it to our current layer\n","        nn.init.uniform(weight, -sampling_range, sampling_range)\n","        \n","        # Similar to above but for the hidden-hidden weights of the current layer\n","        weight = eval('input_lstm.weight_hh_l' + str(ind))\n","        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n","        nn.init.uniform(weight, -sampling_range, sampling_range)\n","        \n","        \n","    # We do the above again, for the backward layer if we are using a bi-directional LSTM (our final model uses this)\n","    if input_lstm.bidirectional:\n","        for ind in range(0, input_lstm.num_layers):\n","            weight = eval('input_lstm.weight_ih_l' + str(ind) + '_reverse')\n","            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n","            nn.init.uniform(weight, -sampling_range, sampling_range)\n","            weight = eval('input_lstm.weight_hh_l' + str(ind) + '_reverse')\n","            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n","            nn.init.uniform(weight, -sampling_range, sampling_range)\n","\n","    # Bias initialization steps\n","    \n","    # We initialize them to zero except for the forget gate bias, which is initialized to 1\n","    if input_lstm.bias:\n","        for ind in range(0, input_lstm.num_layers):\n","            bias = eval('input_lstm.bias_ih_l' + str(ind))\n","            \n","            # Initializing to zero\n","            bias.data.zero_()\n","            \n","            # This is the range of indices for our forget gates for each LSTM cell\n","            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n","            \n","            #Similar for the hidden-hidden layer\n","            bias = eval('input_lstm.bias_hh_l' + str(ind))\n","            bias.data.zero_()\n","            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n","            \n","        # Similar to above, we do for backward layer if we are using a bi-directional LSTM \n","        if input_lstm.bidirectional:\n","            for ind in range(0, input_lstm.num_layers):\n","                bias = eval('input_lstm.bias_ih_l' + str(ind) + '_reverse')\n","                bias.data.zero_()\n","                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n","                bias = eval('input_lstm.bias_hh_l' + str(ind) + '_reverse')\n","                bias.data.zero_()\n","                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1"]},{"cell_type":"markdown","metadata":{},"source":["**Những hàm cho việc sử dụng CRF**"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:32:16.372023Z","iopub.status.busy":"2024-01-15T10:32:16.371751Z","iopub.status.idle":"2024-01-15T10:32:16.384582Z","shell.execute_reply":"2024-01-15T10:32:16.383887Z","shell.execute_reply.started":"2024-01-15T10:32:16.371998Z"},"trusted":true},"outputs":[],"source":["def log_sum_exp(vec):\n","    '''\n","    This function calculates the score explained above for the forward algorithm\n","    vec 2D: 1 * tagset_size\n","    '''\n","    max_score = vec[0, argmax(vec)]\n","    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n","    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n","    \n","def argmax(vec):\n","    '''\n","    This function returns the max index in a vector\n","    '''\n","    _, idx = torch.max(vec, 1)\n","    return to_scalar(idx)\n","\n","def to_scalar(var):\n","    '''\n","    Function to convert pytorch tensor to a scalar\n","    '''\n","    return var.view(-1).data.tolist()[0]"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:32:16.386417Z","iopub.status.busy":"2024-01-15T10:32:16.385807Z","iopub.status.idle":"2024-01-15T10:32:16.397609Z","shell.execute_reply":"2024-01-15T10:32:16.396756Z","shell.execute_reply.started":"2024-01-15T10:32:16.386379Z"},"trusted":true},"outputs":[],"source":["def score_sentences(self, feats, tags):\n","    # tags is ground_truth, a list of ints, length is len(sentence)\n","    # feats is a 2D tensor, len(sentence) * tagset_size\n","    r = torch.LongTensor(range(feats.size()[0]))\n","    if self.use_gpu:\n","        r = r.cuda()\n","        pad_start_tags = torch.cat([torch.cuda.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n","        pad_stop_tags = torch.cat([tags, torch.cuda.LongTensor([self.tag_to_ix[STOP_TAG]])])\n","    else:\n","        pad_start_tags = torch.cat([torch.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n","        pad_stop_tags = torch.cat([tags, torch.LongTensor([self.tag_to_ix[STOP_TAG]])])\n","\n","    score = torch.sum(self.transitions[pad_stop_tags, pad_start_tags]) + torch.sum(feats[r, tags])\n","\n","    return score"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:32:16.398901Z","iopub.status.busy":"2024-01-15T10:32:16.398612Z","iopub.status.idle":"2024-01-15T10:32:16.408506Z","shell.execute_reply":"2024-01-15T10:32:16.407783Z","shell.execute_reply.started":"2024-01-15T10:32:16.398876Z"},"trusted":true},"outputs":[],"source":["def forward_alg(self, feats):\n","    '''\n","    This function performs the forward algorithm\n","    '''\n","    # calculate in log domain\n","    # feats is len(sentence) * tagset_size\n","    # initialize alpha with a Tensor with values all equal to -10000.\n","    \n","    # Do the forward algorithm to compute the partition function\n","    init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n","    \n","    # START_TAG has all of the score.\n","    init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n","    \n","    # Wrap in a variable so that we will get automatic backprop\n","    forward_var = autograd.Variable(init_alphas)\n","    if self.use_gpu:\n","        forward_var = forward_var.cuda()\n","        \n","    # Iterate through the sentence\n","    for feat in feats:\n","        # broadcast the emission score: it is the same regardless of\n","        # the previous tag\n","        emit_score = feat.view(-1, 1)\n","        \n","        # the ith entry of trans_score is the score of transitioning to\n","        # next_tag from i\n","        tag_var = forward_var + self.transitions + emit_score\n","        \n","        # The ith entry of next_tag_var is the value for the\n","        # edge (i -> next_tag) before we do log-sum-exp\n","        max_tag_var, _ = torch.max(tag_var, dim=1)\n","        \n","        # The forward variable for this tag is log-sum-exp of all the\n","        # scores.\n","        tag_var = tag_var - max_tag_var.view(-1, 1)\n","        \n","        # Compute log sum exp in a numerically stable way for the forward algorithm\n","        forward_var = max_tag_var + torch.log(torch.sum(torch.exp(tag_var), dim=1)).view(1, -1) # ).view(1, -1)\n","    terminal_var = (forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]).view(1, -1)\n","    alpha = log_sum_exp(terminal_var)\n","    # Z(x)\n","    return alpha"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:32:16.410167Z","iopub.status.busy":"2024-01-15T10:32:16.409718Z","iopub.status.idle":"2024-01-15T10:32:16.424286Z","shell.execute_reply":"2024-01-15T10:32:16.423149Z","shell.execute_reply.started":"2024-01-15T10:32:16.410133Z"},"trusted":true},"outputs":[],"source":["def viterbi_algo(self, feats):\n","    '''\n","    In this function, we implement the viterbi algorithm.\n","    A Dynamic programming based approach to find the best tag sequence\n","    '''\n","    backpointers = []\n","    # analogous to forward\n","    \n","    # Initialize the viterbi variables in log space\n","    init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n","    init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n","    \n","    # forward_var at step i holds the viterbi variables for step i-1\n","    forward_var = Variable(init_vvars)\n","    if self.use_gpu:\n","        forward_var = forward_var.cuda()\n","    for feat in feats:\n","        next_tag_var = forward_var.view(1, -1).expand(self.tagset_size, self.tagset_size) + self.transitions\n","        _, bptrs_t = torch.max(next_tag_var, dim=1)\n","        bptrs_t = bptrs_t.squeeze().data.cpu().numpy() # holds the backpointers for this step\n","        next_tag_var = next_tag_var.data.cpu().numpy() \n","        viterbivars_t = next_tag_var[range(len(bptrs_t)), bptrs_t] # holds the viterbi variables for this step\n","        viterbivars_t = Variable(torch.FloatTensor(viterbivars_t))\n","        if self.use_gpu:\n","            viterbivars_t = viterbivars_t.cuda()\n","            \n","        # Now add in the emission scores, and assign forward_var to the set\n","        # of viterbi variables we just computed\n","        forward_var = viterbivars_t + feat\n","        backpointers.append(bptrs_t)\n","\n","    # Transition to STOP_TAG\n","    terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","    terminal_var.data[self.tag_to_ix[STOP_TAG]] = -10000.\n","    terminal_var.data[self.tag_to_ix[START_TAG]] = -10000.\n","    best_tag_id = argmax(terminal_var.unsqueeze(0))\n","    path_score = terminal_var[best_tag_id]\n","    \n","    # Follow the back pointers to decode the best path.\n","    best_path = [best_tag_id]\n","    for bptrs_t in reversed(backpointers):\n","        best_tag_id = bptrs_t[best_tag_id]\n","        best_path.append(best_tag_id)\n","        \n","    # Pop off the start tag (we dont want to return that to the caller)\n","    start = best_path.pop()\n","    assert start == self.tag_to_ix[START_TAG] # Sanity check\n","    best_path.reverse()\n","    return path_score, best_path"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:32:16.425838Z","iopub.status.busy":"2024-01-15T10:32:16.425473Z","iopub.status.idle":"2024-01-15T10:32:16.439279Z","shell.execute_reply":"2024-01-15T10:32:16.438434Z","shell.execute_reply.started":"2024-01-15T10:32:16.425809Z"},"trusted":true},"outputs":[],"source":["def forward_calc(self, sentence, chars, chars2_length, d):\n","    \n","    '''\n","    The function calls viterbi decode and generates the \n","    most probable sequence of tags for the sentence\n","    '''\n","    \n","    # Get the emission scores from the BiLSTM\n","    feats = self._get_lstm_features(sentence, chars, chars2_length, d)\n","    # viterbi to get tag_seq\n","    \n","    # Find the best path, given the features.\n","    if self.use_crf:\n","        score, tag_seq = self.viterbi_decode(feats)\n","    else:\n","        score, tag_seq = torch.max(feats, 1)\n","        tag_seq = list(tag_seq.cpu().data)\n","\n","    return score, tag_seq"]},{"cell_type":"markdown","metadata":{},"source":["**Cài đặt mô hình**"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:32:16.440831Z","iopub.status.busy":"2024-01-15T10:32:16.440522Z","iopub.status.idle":"2024-01-15T10:32:16.453939Z","shell.execute_reply":"2024-01-15T10:32:16.452913Z","shell.execute_reply.started":"2024-01-15T10:32:16.440804Z"},"trusted":true},"outputs":[],"source":["def get_lstm_features(self, sentence, chars2, chars2_length, d):\n","\n","    chars_embeds = self.char_embeds(chars2).unsqueeze(1)\n","\n","    ## Creating Character level representation using Convolutional Neural Netowrk\n","    ## followed by a Maxpooling Layer\n","    chars_cnn_out3 = self.char_cnn3(chars_embeds)\n","    chars_embeds = nn.functional.max_pool2d(chars_cnn_out3,\n","                    kernel_size=(chars_cnn_out3.size(2), 1))\n","    chars_embeds = chars_embeds.view(chars_cnn_out3.size(0), self.out_channels)\n","\n","    ## Loading word embeddings\n","    embeds = self.word_embeds(sentence)\n","\n","    ## We concatenate the word embeddings and the character level representation\n","    ## to create unified representation for each word\n","    embeds = torch.cat((embeds, chars_embeds), 1)\n","\n","    embeds = embeds.unsqueeze(1)\n","\n","    ## Dropout on the unified embeddings\n","    embeds = self.dropout(embeds)\n","\n","    ## Word lstm\n","    ## Takes words as input and generates a output at each step\n","    lstm_out, _ = self.lstm(embeds)\n","\n","    ## Reshaping the outputs from the lstm layer\n","    lstm_out = lstm_out.view(len(sentence), self.hidden_dim*2)\n","\n","    # Dropout on the lstm output\n","    lstm_out = self.dropout(lstm_out)\n","\n","    ## Linear layer converts the ouput vectors to tag space\n","    lstm_feats = self.hidden2tag(lstm_out)\n","    \n","    return lstm_feats"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:32:16.455528Z","iopub.status.busy":"2024-01-15T10:32:16.455225Z","iopub.status.idle":"2024-01-15T10:32:16.466443Z","shell.execute_reply":"2024-01-15T10:32:16.465493Z","shell.execute_reply.started":"2024-01-15T10:32:16.455502Z"},"trusted":true},"outputs":[],"source":["def get_neg_log_likelihood(self, sentence, tags, chars2, chars2_length, d):\n","    # sentence, tags is a list of ints\n","    # features is a 2D tensor, len(sentence) * self.tagset_size\n","    feats = self._get_lstm_features(sentence, chars2, chars2_length, d)\n","\n","    if self.use_crf:\n","        forward_score = self._forward_alg(feats)\n","        gold_score = self._score_sentence(feats, tags)\n","        return forward_score - gold_score\n","    else:\n","        tags = Variable(tags)\n","        scores = nn.functional.cross_entropy(feats, tags)\n","        return scores"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:32:16.471833Z","iopub.status.busy":"2024-01-15T10:32:16.471516Z","iopub.status.idle":"2024-01-15T10:32:16.489662Z","shell.execute_reply":"2024-01-15T10:32:16.488949Z","shell.execute_reply.started":"2024-01-15T10:32:16.471806Z"},"trusted":true},"outputs":[],"source":["class Print(nn.Module):\n","    def forward(self, x):\n","        print(x.size())\n","        return x\n","\n","class BiLSTM_CRF(nn.Module):\n","\n","    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim,\n","                 char_to_ix=None, pre_word_embeds=None, char_out_dimension=25,char_embedding_dim=50, use_gpu=False\n","                 , use_crf=True, char_mode='CNN'):\n","        '''\n","        Input parameters:\n","                \n","                vocab_size= Size of vocabulary (int)\n","                tag_to_ix = Dictionary that maps NER tags to indices\n","                embedding_dim = Dimension of word embeddings (int)\n","                hidden_dim = The hidden dimension of the LSTM layer (int)\n","                char_to_ix = Dictionary that maps characters to indices\n","                pre_word_embeds = Numpy array which provides mapping from word embeddings to word indices\n","                char_out_dimension = Output dimension from the CNN encoder for character\n","                char_embedding_dim = Dimension of the character embeddings\n","                use_gpu = defines availability of GPU, \n","                    when True: CUDA function calls are made\n","                    else: Normal CPU function calls are made\n","                use_crf = parameter which decides if you want to use the CRF layer for output decoding\n","        '''\n","        \n","        super(BiLSTM_CRF, self).__init__()\n","        \n","        #parameter initialization for the model\n","        self.use_gpu = use_gpu\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.vocab_size = vocab_size\n","        self.tag_to_ix = tag_to_ix\n","        self.use_crf = use_crf\n","        self.tagset_size = len(tag_to_ix)\n","        self.out_channels = char_out_dimension\n","        self.char_mode = char_mode\n","\n","        if char_embedding_dim is not None:\n","            self.char_embedding_dim = char_embedding_dim\n","            \n","            #Initializing the character embedding layer\n","            self.char_embeds = nn.Embedding(len(char_to_ix), char_embedding_dim)\n","            init_embedding(self.char_embeds.weight)\n","                \n","            #Performing CNN encoding on the character embeddings\n","                \n","            layers = []\n","\n","            layers.append(nn.Conv2d(in_channels=1, out_channels=25, kernel_size=(3, char_embedding_dim), padding=(2,0)))\n","\n","            layers.append(nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size=(3, 1), padding_mode='replicate'))\n","\n","            net = nn.Sequential(*layers)\n","\n","            self.char_cnn3 = net\n","\n","        #Creating Embedding layer with dimension of ( number of words * dimension of each word)\n","        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n","        if pre_word_embeds is not None:\n","            #Initializes the word embeddings with pretrained word embeddings\n","            self.pre_word_embeds = True\n","            self.word_embeds.weight = nn.Parameter(torch.FloatTensor(pre_word_embeds))\n","        else:\n","            self.pre_word_embeds = False\n","    \n","        #Initializing the dropout layer, with dropout specificed in parameters\n","        self.dropout = nn.Dropout(parameters['dropout'])\n","        \n","        #Lstm Layer:\n","        #input dimension: word embedding dimension + character level representation\n","        #bidirectional=True, specifies that we are using the bidirectional LSTM\n","        self.lstm = nn.LSTM(embedding_dim+self.out_channels, hidden_dim, 2, bidirectional=True)\n","        \n","        \n","        #Initializing the lstm layer using predefined function for initialization\n","        init_lstm(self.lstm)\n","        \n","        # Linear layer which maps the output of the bidirectional LSTM into tag space.\n","        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n","        \n","        #Initializing the linear layer using predefined function for initialization\n","        init_linear(self.hidden2tag) \n","\n","        if self.use_crf:\n","            # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n","            # Matrix has a dimension of (total number of tags * total number of tags)\n","            self.transitions = nn.Parameter(\n","                torch.zeros(self.tagset_size, self.tagset_size))\n","            \n","            # These two statements enforce the constraint that we never transfer\n","            # to the start tag and we never transfer from the stop tag\n","            self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n","            self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n","\n","    #assigning the functions, which we have defined earlier\n","    _score_sentence = score_sentences\n","    _get_lstm_features = get_lstm_features\n","    _forward_alg = forward_alg\n","    viterbi_decode = viterbi_algo\n","    neg_log_likelihood = get_neg_log_likelihood\n","    forward = forward_calc"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:32:16.491084Z","iopub.status.busy":"2024-01-15T10:32:16.490782Z","iopub.status.idle":"2024-01-15T10:32:16.602395Z","shell.execute_reply":"2024-01-15T10:32:16.601384Z","shell.execute_reply.started":"2024-01-15T10:32:16.491057Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_42/3399668671.py:6: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n","  nn.init.uniform(input_embedding, -bias, bias)\n","/tmp/ipykernel_42/2828957834.py:25: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n","  nn.init.uniform(weight, -sampling_range, sampling_range)\n","/tmp/ipykernel_42/2828957834.py:30: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n","  nn.init.uniform(weight, -sampling_range, sampling_range)\n","/tmp/ipykernel_42/2828957834.py:38: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n","  nn.init.uniform(weight, -sampling_range, sampling_range)\n","/tmp/ipykernel_42/2828957834.py:41: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n","  nn.init.uniform(weight, -sampling_range, sampling_range)\n"]}],"source":["#creating the model using the Class defined above\n","model = BiLSTM_CRF(vocab_size=len(word_to_id),\n","                   tag_to_ix=tag_to_id,\n","                   embedding_dim=parameters['word_dim'],\n","                   hidden_dim=parameters['word_lstm_dim'],\n","                   use_gpu=use_gpu,\n","                   char_to_ix=char_to_id,\n","                   pre_word_embeds=word_embeds,\n","                   use_crf=parameters['crf'],\n","                   char_mode=parameters['char_mode'])"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:32:16.604170Z","iopub.status.busy":"2024-01-15T10:32:16.603748Z","iopub.status.idle":"2024-01-15T10:32:16.910009Z","shell.execute_reply":"2024-01-15T10:32:16.909005Z","shell.execute_reply.started":"2024-01-15T10:32:16.604130Z"},"trusted":true},"outputs":[{"data":{"text/plain":["BiLSTM_CRF(\n","  (char_embeds): Embedding(180, 50)\n","  (char_cnn3): Sequential(\n","    (0): Conv2d(1, 25, kernel_size=(3, 50), stride=(1, 1), padding=(2, 0))\n","    (1): Conv2d(25, 25, kernel_size=(3, 1), stride=(1, 1), padding_mode=replicate)\n","  )\n","  (word_embeds): Embedding(5728, 100)\n","  (dropout): Dropout(p=0.25, inplace=False)\n","  (lstm): LSTM(125, 200, num_layers=2, bidirectional=True)\n","  (hidden2tag): Linear(in_features=400, out_features=23, bias=True)\n",")"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["model.cuda()"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:32:16.912022Z","iopub.status.busy":"2024-01-15T10:32:16.911474Z","iopub.status.idle":"2024-01-15T10:32:16.917821Z","shell.execute_reply":"2024-01-15T10:32:16.916918Z","shell.execute_reply.started":"2024-01-15T10:32:16.911983Z"},"trusted":true},"outputs":[],"source":["learning_rate = 0.001\n","number_of_epochs = parameters['epoch'] \n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:32:16.919803Z","iopub.status.busy":"2024-01-15T10:32:16.919308Z","iopub.status.idle":"2024-01-15T10:51:00.363990Z","shell.execute_reply":"2024-01-15T10:51:00.362905Z","shell.execute_reply.started":"2024-01-15T10:32:16.919730Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epochs: 2 | Loss:  7.023 | Val_Loss:  2.894 \n","Saving Model to  ./models/BCC_syllable\n","Epochs: 3 | Loss:  1.556 | Val_Loss:  2.187 \n","Saving Model to  ./models/BCC_syllable\n","Epochs: 4 | Loss:  0.968 | Val_Loss:  2.216 \n","Epochs: 5 | Loss:  0.667 | Val_Loss:  1.815 \n","Saving Model to  ./models/BCC_syllable\n","Epochs: 6 | Loss:  0.518 | Val_Loss:  1.800 \n","Saving Model to  ./models/BCC_syllable\n","Epochs: 7 | Loss:  0.417 | Val_Loss:  1.970 \n","Epochs: 8 | Loss:  0.322 | Val_Loss:  2.129 \n","Epochs: 9 | Loss:  0.263 | Val_Loss:  2.405 \n","Epochs: 10 | Loss:  0.232 | Val_Loss:  2.354 \n","Epochs: 11 | Loss:  0.182 | Val_Loss:  2.769 \n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["def train_loop(model):\n","    tr = time.time()\n","    model.train(True)\n","    eval_every = len(train_data)\n","    plot_every = 2000\n","    count = 0\n","    min_val_loss = 1000\n","    for epoch in range(1,number_of_epochs):\n","        total_loss_train = 0.0\n","        for i, index in enumerate(np.random.permutation(len(train_data))):\n","            data = train_data[index]\n","\n","            ##gradient updates for each data entry\n","            model.zero_grad()\n","\n","            sentence_in = data['words']\n","            sentence_in = Variable(torch.LongTensor(sentence_in))\n","            tags = data['tags']\n","            chars2 = data['chars']\n","\n","            d = {}\n","\n","            ## Padding the each word to max word size of that sentence\n","            chars2_length = [len(c) for c in chars2]\n","            char_maxl = max(chars2_length)\n","            chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n","            for i, c in enumerate(chars2):\n","                chars2_mask[i, :chars2_length[i]] = c\n","            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n","\n","\n","            targets = torch.LongTensor(tags)\n","\n","            #we calculate the negative log-likelihood for the predicted tags using the predefined function\n","            if use_gpu:\n","                neg_log_likelihood = model.neg_log_likelihood(sentence_in.cuda(), targets.cuda(), chars2_mask.cuda(), chars2_length, d)\n","            else:\n","                neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets, chars2_mask, chars2_length, d)\n","            total_loss_train += neg_log_likelihood.data\n","            neg_log_likelihood.backward()\n","\n","    #         #we use gradient clipping to avoid exploding gradients\n","    #         torch.nn.utils.clip_grad_norm(model.parameters(), gradient_clip)\n","            optimizer.step()\n","\n","        total_loss_val=0.0\n","        model.train(False)\n","        for i, index in enumerate(np.random.permutation(len(dev_data))):\n","            data_val = dev_data[index]\n","\n","            sentence_in = data_val['words']\n","            sentence_in = Variable(torch.LongTensor(sentence_in))\n","            tags = data_val['tags']\n","            chars2 = data_val['chars']\n","\n","            d = {}\n","\n","            ## Padding the each word to max word size of that sentence\n","            chars2_length = [len(c) for c in chars2]\n","            char_maxl = max(chars2_length)\n","            chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n","            for i, c in enumerate(chars2):\n","                chars2_mask[i, :chars2_length[i]] = c\n","            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n","\n","\n","            targets = torch.LongTensor(tags)\n","\n","            #we calculate the negative log-likelihood for the predicted tags using the predefined function\n","            if use_gpu:\n","                neg_log_likelihood = model.neg_log_likelihood(sentence_in.cuda(), targets.cuda(), chars2_mask.cuda(), chars2_length, d)\n","            else:\n","                neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets, chars2_mask, chars2_length, d)\n","            total_loss_val += neg_log_likelihood.data\n","\n","        print(\n","            f'Epochs: {epoch + 1} | Loss: {total_loss_train/len(train_data): .3f} | Val_Loss: {total_loss_val / len(dev_data): .3f} ')\n","        if total_loss_val / len(dev_data) < min_val_loss:\n","            min_val_loss = total_loss_val / len(dev_data)\n","            count = epoch\n","            print(\"Saving Model to \", model_name)\n","            torch.save(model.state_dict(), model_name)\n","        if epoch - count >= 5:\n","            return\n","\n","        model.train(True)\n","\n","\n","    print(time.time() - tr)\n","\n","train_loop(model)\n","\n","#load pre-trained model\n","model.load_state_dict(torch.load(model_name))"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluate"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:51:00.366021Z","iopub.status.busy":"2024-01-15T10:51:00.365535Z","iopub.status.idle":"2024-01-15T10:51:34.084729Z","shell.execute_reply":"2024-01-15T10:51:34.083663Z","shell.execute_reply.started":"2024-01-15T10:51:00.365984Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                     precision    recall  f1-score   support\n","\n","                AGE      0.967     0.967     0.967       582\n","               DATE      0.980     0.985     0.983      1654\n","             GENDER      0.955     0.961     0.958       462\n","                JOB      0.734     0.543     0.625       173\n","           LOCATION      0.932     0.921     0.927      4441\n","               NAME      0.944     0.843     0.890       318\n","       ORGANIZATION      0.840     0.838     0.839       771\n","         PATIENT_ID      0.982     0.971     0.976      2005\n","SYMPTOM_AND_DISEASE      0.897     0.739     0.811      1136\n","     TRANSPORTATION      0.896     0.933     0.914       193\n","\n","          micro avg      0.939     0.912     0.925     11735\n","          macro avg      0.913     0.870     0.889     11735\n","       weighted avg      0.937     0.912     0.924     11735\n","\n"]}],"source":["def evaluate(model, test_data, id_to_tag):\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if use_cuda:\n","        model = model.cuda()\n","\n","    predictions = []\n","    labels = []\n","    \n","    for data in test_data:\n","\n","        words = data['str_words']\n","        chars2 = data['chars']\n","\n","        d = {} \n","\n","        # Padding the each word to max word size of that sentence\n","        chars2_length = [len(c) for c in chars2]\n","        char_maxl = max(chars2_length)\n","        chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n","        for i, c in enumerate(chars2):\n","            chars2_mask[i, :chars2_length[i]] = c\n","        chars2_mask = Variable(torch.LongTensor(chars2_mask))\n","\n","        dwords = Variable(torch.LongTensor(data['words']))\n","\n","        # We are getting the predicted output from our model\n","        if use_gpu:\n","            _,predicted = model(dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n","            predictions.append([id_to_tag[val] for val in predicted])\n","            labels.append([id_to_tag[val] for val in data['tags']])\n","    print(classification_report(y_pred=predictions, y_true=labels, digits=3))\n","\n","evaluate(model, test_data, id_to_tag)"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:51:34.086894Z","iopub.status.busy":"2024-01-15T10:51:34.086435Z","iopub.status.idle":"2024-01-15T10:51:34.099145Z","shell.execute_reply":"2024-01-15T10:51:34.098172Z","shell.execute_reply.started":"2024-01-15T10:51:34.086849Z"},"trusted":true},"outputs":[],"source":["def ner(model, sentence, id_to_tag):\n","\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if use_cuda:\n","        model = model.cuda()\n","\n","    #preprocessing\n","    s=sentence.split()\n","    str_words = [w for w in s]\n","    words = [word_to_id[w if w in word_to_id else '<UNK>'] for w in str_words]\n","\n","    # Skip characters that are not in the training set\n","    chars = [[char_to_id[c] for c in w if c in char_to_id] for w in str_words]\n","\n","    final_test={\n","        'str_words': str_words,\n","        'words': words,\n","        'chars': chars,\n","    }\n","\n","    #prediction\n","    predictions = []\n","    words = final_test['str_words']\n","    chars2 = final_test['chars']\n","\n","    d = {} \n","\n","    # Padding the each word to max word size of that sentence\n","    chars2_length = [len(c) for c in chars2]\n","    char_maxl = max(chars2_length)\n","    chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n","    for i, c in enumerate(chars2):\n","        chars2_mask[i, :chars2_length[i]] = c\n","    chars2_mask = Variable(torch.LongTensor(chars2_mask))\n","\n","    dwords = Variable(torch.LongTensor(final_test['words']))\n","\n","    # We are getting the predicted output from our model\n","    _,predicted_id = model(dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n","\n","    prediction_label = [id_to_tag[val] for val in predicted_id]\n","\n","    print(sentence)\n","    print(prediction_label)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T10:51:34.100961Z","iopub.status.busy":"2024-01-15T10:51:34.100593Z","iopub.status.idle":"2024-01-15T10:51:34.129208Z","shell.execute_reply":"2024-01-15T10:51:34.128237Z","shell.execute_reply.started":"2024-01-15T10:51:34.100932Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Bệnh nhân nhập viện tối qua ở Bệnh Viện 115 là bệnh nhân thứ 82 , di chuyển qua nhiều thành phố bằng xe biển hiệu E-402\n","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'O', 'O', 'O', 'O', 'B-PATIENT_ID', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TRANSPORTATION']\n"]}],"source":["ner(model,\n","    'Bệnh nhân nhập viện tối qua ở Bệnh Viện 115 là bệnh nhân thứ 82 , di chuyển qua nhiều thành phố bằng xe biển hiệu E-402',\n","    id_to_tag)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3960915,"sourceId":6895280,"sourceType":"datasetVersion"},{"datasetId":4264698,"sourceId":7344525,"sourceType":"datasetVersion"},{"datasetId":4264824,"sourceId":7344720,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
