{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6895280,"sourceType":"datasetVersion","datasetId":3960915}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#tải các thư viện cần thiết\n!pip install transformer\n!pip install seqeval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-07T16:34:54.067250Z","iopub.execute_input":"2024-01-07T16:34:54.067573Z","iopub.status.idle":"2024-01-07T16:35:13.438716Z","shell.execute_reply.started":"2024-01-07T16:34:54.067542Z","shell.execute_reply":"2024-01-07T16:35:13.437519Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement transformer (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for transformer\u001b[0m\u001b[31m\n\u001b[0mCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.24.3)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=52e4a3108388eab9038ffdece3c60cf8e87236c00c88837028ae2d2bc8e4252c\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"}]},{"cell_type":"code","source":"#import các thư viện cần thiết\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\n#Embedding\nfrom transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification\n\n#Modeling\nfrom torch.utils.data import DataLoader\nfrom torch.optim import SGD, Adam\nfrom seqeval.metrics import classification_report\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:35:13.440714Z","iopub.execute_input":"2024-01-07T16:35:13.441021Z","iopub.status.idle":"2024-01-07T16:35:17.843454Z","shell.execute_reply.started":"2024-01-07T16:35:13.440996Z","shell.execute_reply":"2024-01-07T16:35:17.842345Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:35:17.844537Z","iopub.execute_input":"2024-01-07T16:35:17.844991Z","iopub.status.idle":"2024-01-07T16:35:20.367503Z","shell.execute_reply.started":"2024-01-07T16:35:17.844963Z","shell.execute_reply":"2024-01-07T16:35:20.366631Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d161a8251a141dd8931e3b3f03475a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69809fc74c5b4dcfa49f9d787211e007"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0245f81a6392447b943f3ae494c719d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12db707fa63d4877b0a4a777f746f85e"}},"metadata":{}}]},{"cell_type":"code","source":"#hàm đọc dữ liệu\ndef read_dataset(file_path):\n    tokens=[]\n    ner_tags=[]\n    ids=[]\n    count=1\n    with open(file_path) as f:\n        lines=f.readlines()\n        ts=[]\n        nts=[]\n        for line in lines:\n            line = line.split()\n            if len(line)==0:\n                ids.append(count)\n                tokens.append(ts)\n                ner_tags.append(nts)\n                ts=[]\n                nts=[]\n                count+=1\n            else:\n                ts.append(line[0])\n                nts.append(line[-1])\n    data = pd.DataFrame({'Id':ids, 'NER_tags':ner_tags, 'Tokens':tokens})\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:35:20.370273Z","iopub.execute_input":"2024-01-07T16:35:20.371070Z","iopub.status.idle":"2024-01-07T16:35:20.381075Z","shell.execute_reply.started":"2024-01-07T16:35:20.371028Z","shell.execute_reply":"2024-01-07T16:35:20.379930Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_NER_labels(data):\n    NERs = list(data['NER_tags'].values)\n    labels_list = []\n    for value in NERs:\n        labels_list = labels_list + value\n    types = list(set(labels_list))\n    return types","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:35:20.382384Z","iopub.execute_input":"2024-01-07T16:35:20.382724Z","iopub.status.idle":"2024-01-07T16:35:20.391361Z","shell.execute_reply.started":"2024-01-07T16:35:20.382692Z","shell.execute_reply":"2024-01-07T16:35:20.390500Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_df = read_dataset('/kaggle/input/covid19vi/word/train_word.conll')\nval_df = read_dataset('/kaggle/input/covid19vi/word/dev_word.conll')","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:35:20.392513Z","iopub.execute_input":"2024-01-07T16:35:20.392818Z","iopub.status.idle":"2024-01-07T16:35:20.643853Z","shell.execute_reply.started":"2024-01-07T16:35:20.392792Z","shell.execute_reply":"2024-01-07T16:35:20.642926Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:35:20.645094Z","iopub.execute_input":"2024-01-07T16:35:20.645402Z","iopub.status.idle":"2024-01-07T16:35:20.670267Z","shell.execute_reply.started":"2024-01-07T16:35:20.645377Z","shell.execute_reply":"2024-01-07T16:35:20.669150Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   Id                                           NER_tags  \\\n0   1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n1   2  [O, O, O, O, O, O, O, B-SYMPTOM_AND_DISEASE, I...   \n2   3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n3   4  [O, O, O, O, O, O, B-LOCATION, O, B-LOCATION, ...   \n4   5  [O, O, B-PATIENT_ID, O, O, O, O, O, O, B-PATIE...   \n\n                                              Tokens  \n0  [Đồng_thời, ,, bệnh_viện, tiếp_tục, thực_hiện,...  \n1  [\", Số, bệnh_viện, có_thể, tiếp_nhận, bệnh_nhâ...  \n2  [Ngoài_ra, ,, những, người, tiếp_xúc, gián_tiế...  \n3  [Bà, này, khi, trở, về, quá_cảnh, Doha, (, Qat...  \n4  [\", Bệnh_nhân, 523, \", và, chồng, là, \", bệnh_...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>NER_tags</th>\n      <th>Tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[Đồng_thời, ,, bệnh_viện, tiếp_tục, thực_hiện,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[O, O, O, O, O, O, O, B-SYMPTOM_AND_DISEASE, I...</td>\n      <td>[\", Số, bệnh_viện, có_thể, tiếp_nhận, bệnh_nhâ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[Ngoài_ra, ,, những, người, tiếp_xúc, gián_tiế...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[O, O, O, O, O, O, B-LOCATION, O, B-LOCATION, ...</td>\n      <td>[Bà, này, khi, trở, về, quá_cảnh, Doha, (, Qat...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[O, O, B-PATIENT_ID, O, O, O, O, O, O, B-PATIE...</td>\n      <td>[\", Bệnh_nhân, 523, \", và, chồng, là, \", bệnh_...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"unique_labels = get_NER_labels(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:35:20.671433Z","iopub.execute_input":"2024-01-07T16:35:20.674026Z","iopub.status.idle":"2024-01-07T16:35:22.463560Z","shell.execute_reply.started":"2024-01-07T16:35:20.673984Z","shell.execute_reply":"2024-01-07T16:35:22.462768Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **TẠO LỚP DATASET BẰNG PYTORCH DATASET**","metadata":{}},{"cell_type":"code","source":"def align_label(text, labels, flag=False):\n    label_all_tokens = flag #flag xác định cách thực hiện align_label\n    \n    tokenized_input = tokenizer(text, padding='max_length', max_length=256, truncation=True, is_split_into_words=True)\n    word_ids = tokenized_input.input_ids\n    \n    start_part = True\n    label_ids = []\n    count = 0\n    \n    for i in range(len(word_ids)):\n        \n        if word_ids[i] == 0 or word_ids[i] == 1 or word_ids[i] == 2:\n            label_ids.append(-100)\n            \n        elif count < len(text) and ''.join(tokenizer.decode(tokenized_input['input_ids'][i]).split()) == text[count]:\n            label_ids.append(labels_to_ids[labels[count]])\n            count+=1\n            start_part = True\n        else:\n            if start_part:\n                label_ids.append(labels_to_ids[labels[count]])\n                count+=1\n                start_part = False\n            else:\n                label_ids.append(labels_to_ids[labels[count]] if label_all_tokens else -100)\n                \n    return label_ids\n\nclass DataSet(torch.utils.data.Dataset):\n\n    def __init__(self, df, flag_align_label=False):\n\n        lb = df['NER_tags'].values.tolist()\n        txt = df['Tokens'].values.tolist()\n        self.texts = [tokenizer(i, padding='max_length', max_length = 256,\n                                truncation=True, return_tensors=\"pt\", is_split_into_words=True) for i in txt]\n        self.labels = [align_label(i,j,flag_align_label) for i,j in zip(txt, lb)]\n        \n    def __len__(self):\n\n        return len(self.labels)\n\n    def get_data(self, idx):\n        return self.texts[idx]\n\n    def get_labels(self, idx):\n        return torch.LongTensor(self.labels[idx])\n\n    def __getitem__(self, idx):\n        data = self.get_data(idx)\n        labels = self.get_labels(idx)\n\n        return data, labels","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:35:22.464551Z","iopub.execute_input":"2024-01-07T16:35:22.464809Z","iopub.status.idle":"2024-01-07T16:35:22.477528Z","shell.execute_reply.started":"2024-01-07T16:35:22.464786Z","shell.execute_reply":"2024-01-07T16:35:22.476500Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Tạo 2 dictionary để xác định id nào sẽ là label nào và ngược lại.**","metadata":{}},{"cell_type":"code","source":"labels_to_ids = {k: v for v, k in enumerate(unique_labels)}\nids_to_labels = {v: k for v, k in enumerate(unique_labels)}","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:35:22.480584Z","iopub.execute_input":"2024-01-07T16:35:22.481192Z","iopub.status.idle":"2024-01-07T16:35:22.494185Z","shell.execute_reply.started":"2024-01-07T16:35:22.481164Z","shell.execute_reply":"2024-01-07T16:35:22.493300Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"labels_to_ids","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:35:22.495293Z","iopub.execute_input":"2024-01-07T16:35:22.495616Z","iopub.status.idle":"2024-01-07T16:35:22.506344Z","shell.execute_reply.started":"2024-01-07T16:35:22.495591Z","shell.execute_reply":"2024-01-07T16:35:22.505490Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'B-DATE': 0,\n 'I-JOB': 1,\n 'B-PATIENT_ID': 2,\n 'I-LOCATION': 3,\n 'O': 4,\n 'I-SYMPTOM_AND_DISEASE': 5,\n 'I-TRANSPORTATION': 6,\n 'B-NAME': 7,\n 'B-AGE': 8,\n 'I-ORGANIZATION': 9,\n 'B-ORGANIZATION': 10,\n 'B-GENDER': 11,\n 'I-NAME': 12,\n 'B-TRANSPORTATION': 13,\n 'I-AGE': 14,\n 'I-DATE': 15,\n 'B-SYMPTOM_AND_DISEASE': 16,\n 'I-PATIENT_ID': 17,\n 'B-LOCATION': 18,\n 'B-JOB': 19}"},"metadata":{}}]},{"cell_type":"markdown","source":"# **BUILD MODEL PHOBERT FOR TOKEN CLASSIFICATION**","metadata":{}},{"cell_type":"code","source":"class PhoBertModel(torch.nn.Module):\n\n    def __init__(self):\n\n        super(PhoBertModel, self).__init__()\n\n        self.phobert = AutoModelForTokenClassification.from_pretrained(\"vinai/phobert-base\", num_labels=len(unique_labels))\n\n    def forward(self, input_id, mask, label):\n        \n        output = self.phobert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:35:22.507698Z","iopub.execute_input":"2024-01-07T16:35:22.508023Z","iopub.status.idle":"2024-01-07T16:35:22.515356Z","shell.execute_reply.started":"2024-01-07T16:35:22.507998Z","shell.execute_reply":"2024-01-07T16:35:22.514546Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# **TRAIN MODEL**","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:35:22.516668Z","iopub.execute_input":"2024-01-07T16:35:22.517465Z","iopub.status.idle":"2024-01-07T16:35:22.528629Z","shell.execute_reply.started":"2024-01-07T16:35:22.517429Z","shell.execute_reply":"2024-01-07T16:35:22.527666Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def train_loop(model, train_df, val_df, flag_align_label):\n\n    train_dataset = DataSet(train_df, flag_align_label)\n    val_dataset = DataSet(val_df, flag_align_label)\n\n    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n    val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n\n    if use_cuda:\n        model = model.cuda()\n        \n    min_val_loss = 1000\n    count = 0\n    \n    for epoch_num in range(EPOCHS):\n\n        total_acc_train = 0\n        total_loss_train = 0\n\n        model.train()\n\n        for train_data, train_label in tqdm(train_dataloader):\n\n            train_label = train_label.to(device)\n            mask = train_data['attention_mask'].squeeze(1).to(device)\n            input_id = train_data['input_ids'].squeeze(1).to(device)\n\n            optimizer.zero_grad()\n            loss, logits = model(input_id, mask, train_label)\n\n            for i in range(logits.shape[0]):\n\n                logits_clean = logits[i][train_label[i] != -100]\n                label_clean = train_label[i][train_label[i] != -100]\n\n                predictions = logits_clean.argmax(dim=1)\n                acc = (predictions == label_clean).float().mean()\n                total_acc_train += acc\n                total_loss_train += loss.item()\n\n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n\n        total_acc_val = 0\n        total_loss_val = 0\n\n        for val_data, val_label in val_dataloader:\n\n            val_label = val_label.to(device)\n            mask = val_data['attention_mask'].squeeze(1).to(device)\n            input_id = val_data['input_ids'].squeeze(1).to(device)\n\n            loss, logits = model(input_id, mask, val_label)\n\n            for i in range(logits.shape[0]):\n\n                logits_clean = logits[i][val_label[i] != -100]\n                label_clean = val_label[i][val_label[i] != -100]\n\n                predictions = logits_clean.argmax(dim=1)\n                acc = (predictions == label_clean).float().mean()\n                total_acc_val += acc\n                total_loss_val += loss.item()\n\n        val_accuracy = total_acc_val / len(val_df)\n        val_loss = total_loss_val / len(val_df)\n\n        print(\n            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(train_df): .3f} | Accuracy: {total_acc_train / len(train_df): .3f} | Val_Loss: {total_loss_val / len(val_df): .3f} | Accuracy: {total_acc_val / len(val_df): .3f}')\n        if val_loss < min_val_loss:\n            min_val_loss = val_loss\n            torch.save(model.state_dict(), 'phobert_base_ner')\n            count = epoch_num\n        if epoch_num - count >= 5:\n            return\n        \nLEARNING_RATE = 5e-5\nEPOCHS = 30\nBATCH_SIZE = 32\n\nmodel = PhoBertModel()\ntrain_loop(model, train_df, val_df, False)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:35:22.530306Z","iopub.execute_input":"2024-01-07T16:35:22.530700Z","iopub.status.idle":"2024-01-07T16:56:27.321439Z","shell.execute_reply.started":"2024-01-07T16:35:22.530666Z","shell.execute_reply":"2024-01-07T16:56:27.320059Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8496e943df9148e8a85a40d0d1cdd7a7"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|██████████| 158/158 [02:00<00:00,  1.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 1 | Loss:  0.495 | Accuracy:  0.898 | Val_Loss:  0.116 | Accuracy:  0.976\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [02:00<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 2 | Loss:  0.081 | Accuracy:  0.985 | Val_Loss:  0.073 | Accuracy:  0.984\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [02:00<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 3 | Loss:  0.048 | Accuracy:  0.991 | Val_Loss:  0.070 | Accuracy:  0.984\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [02:00<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 4 | Loss:  0.033 | Accuracy:  0.993 | Val_Loss:  0.062 | Accuracy:  0.985\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [02:00<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 5 | Loss:  0.025 | Accuracy:  0.995 | Val_Loss:  0.066 | Accuracy:  0.986\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [02:00<00:00,  1.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 6 | Loss:  0.023 | Accuracy:  0.996 | Val_Loss:  0.073 | Accuracy:  0.984\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [02:00<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 7 | Loss:  0.018 | Accuracy:  0.996 | Val_Loss:  0.070 | Accuracy:  0.985\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [02:00<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 8 | Loss:  0.014 | Accuracy:  0.997 | Val_Loss:  0.075 | Accuracy:  0.985\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [02:00<00:00,  1.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 9 | Loss:  0.011 | Accuracy:  0.998 | Val_Loss:  0.072 | Accuracy:  0.985\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **EVALUATE MODEL**","metadata":{}},{"cell_type":"code","source":"test_df = read_dataset('/kaggle/input/covid19vi/word/test_word.conll')","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:56:27.324687Z","iopub.execute_input":"2024-01-07T16:56:27.325634Z","iopub.status.idle":"2024-01-07T16:56:27.435201Z","shell.execute_reply.started":"2024-01-07T16:56:27.325602Z","shell.execute_reply":"2024-01-07T16:56:27.434351Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, test_df, flag_align_label, ids_to_labels):\n\n    test_dataset = DataSet(test_df, flag_align_label)\n\n    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    if use_cuda:\n        model = model.cuda()\n\n    predictions = []\n    labels = []\n    \n    for test_data, test_label in test_dataloader:\n\n            test_label = test_label.to(device)\n            mask = test_data['attention_mask'].squeeze(1).to(device)\n\n            input_id = test_data['input_ids'].squeeze(1).to(device)\n\n            loss, logits = model(input_id, mask, test_label)\n\n            for i in range(logits.shape[0]):\n                cleaned_logits = logits[i][test_label[i] != -100].argmax(dim=1)\n                predictions.append([ids_to_labels[val.item()] for val in cleaned_logits])\n                cleaned_labels = test_label[i][test_label[i] != -100]\n                labels.append([ids_to_labels[val.item()] for val in cleaned_labels])\n    print(classification_report(y_pred=predictions, y_true=labels, digits=3))\n\nevaluate(model, test_df, False, ids_to_labels)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:56:27.436213Z","iopub.execute_input":"2024-01-07T16:56:27.436489Z","iopub.status.idle":"2024-01-07T16:57:31.349057Z","shell.execute_reply.started":"2024-01-07T16:56:27.436465Z","shell.execute_reply":"2024-01-07T16:57:31.347778Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"                     precision    recall  f1-score   support\n\n                AGE      0.978     0.961     0.970       568\n               DATE      0.981     0.990     0.985      1641\n             GENDER      0.970     0.978     0.974       458\n                JOB      0.773     0.846     0.808       169\n           LOCATION      0.946     0.953     0.949      4346\n               NAME      0.928     0.943     0.935       314\n       ORGANIZATION      0.859     0.900     0.879       768\n         PATIENT_ID      0.982     0.984     0.983      1982\nSYMPTOM_AND_DISEASE      0.875     0.871     0.873      1135\n     TRANSPORTATION      0.944     0.984     0.964       189\n\n          micro avg      0.943     0.952     0.948     11570\n          macro avg      0.924     0.941     0.932     11570\n       weighted avg      0.944     0.952     0.948     11570\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **INFERENCE**","metadata":{}},{"cell_type":"code","source":"def align_word_ids(text, flag):\n    label_all_tokens = flag\n    \n    text = text.split()\n  \n    tokenized_inputs = tokenizer(text, padding='max_length', max_length=256, truncation=True, is_split_into_words=True)\n\n    word_ids = tokenized_inputs.input_ids\n\n    start_part = True\n    label_ids = []\n    count = 0\n    \n    for i in range(len(word_ids)):\n        \n        if word_ids[i] == 0 or word_ids[i] == 1 or word_ids[i] == 2:\n            label_ids.append(-100)\n            \n        elif count < len(text) and ''.join(tokenizer.decode(tokenized_inputs['input_ids'][i]).split()) == text[count]:\n            label_ids.append(1)\n            count+=1\n            start_part = True\n        else:\n            if start_part:\n                label_ids.append(1)\n                count+=1\n                start_part = False\n            else:\n                label_ids.append(1 if label_all_tokens else -100)           \n    return label_ids","metadata":{"execution":{"iopub.status.busy":"2024-01-07T17:34:14.106787Z","iopub.execute_input":"2024-01-07T17:34:14.107541Z","iopub.status.idle":"2024-01-07T17:34:14.115974Z","shell.execute_reply.started":"2024-01-07T17:34:14.107500Z","shell.execute_reply":"2024-01-07T17:34:14.115058Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def ner(model, sentence, flag_align_label):\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n    if use_cuda:\n        model = model.cuda()\n    text = tokenizer(sentence, padding='max_length', max_length = 256, truncation=True, return_tensors=\"pt\")\n\n    mask = text['attention_mask'].to(device)\n    input_id = text['input_ids'].to(device)\n    label_ids = torch.Tensor(align_word_ids(sentence, flag_align_label)).unsqueeze(0).to(device)\n\n    logits = model(input_id, mask, None)\n    logits_clean = logits[0][label_ids != -100]\n\n    predictions = logits_clean.argmax(dim=1).tolist()\n    prediction_label = [ids_to_labels[i] for i in predictions]\n    print(sentence)\n    print(prediction_label)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:57:31.364131Z","iopub.execute_input":"2024-01-07T16:57:31.364486Z","iopub.status.idle":"2024-01-07T16:57:31.376589Z","shell.execute_reply.started":"2024-01-07T16:57:31.364458Z","shell.execute_reply":"2024-01-07T16:57:31.375661Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"ner(model,\n    'Bệnh_nhân nhập_viện tối_qua ở Bệnh_Viện 115 là bệnh_nhân thứ 82 , di_chuyển qua nhiều thành_phố bằng xe biển_hiệu E-402',\n    flag_align_label=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T17:34:17.611439Z","iopub.execute_input":"2024-01-07T17:34:17.612547Z","iopub.status.idle":"2024-01-07T17:34:17.647283Z","shell.execute_reply.started":"2024-01-07T17:34:17.612507Z","shell.execute_reply":"2024-01-07T17:34:17.646142Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Bệnh_nhân nhập_viện tối_qua ở Bệnh_Viện 115 là bệnh_nhân thứ 82 , di_chuyển qua nhiều thành_phố bằng xe biển_hiệu E-402\n['O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'O', 'O', 'B-PATIENT_ID', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TRANSPORTATION']\n","output_type":"stream"}]}]}