{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6892108,"sourceType":"datasetVersion","datasetId":3959307},{"sourceId":6895280,"sourceType":"datasetVersion","datasetId":3960915}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#tải các thư viện cần thiết\n!pip install transformer\n!pip install seqeval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-12T10:15:10.761231Z","iopub.execute_input":"2024-01-12T10:15:10.761596Z","iopub.status.idle":"2024-01-12T10:15:24.543103Z","shell.execute_reply.started":"2024-01-12T10:15:10.761567Z","shell.execute_reply":"2024-01-12T10:15:24.541974Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement transformer (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for transformer\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: seqeval in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.24.3)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"#import các thư viện cần thiết\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\n#Embedding\nfrom transformers import BertTokenizerFast, BertForTokenClassification, BertModel\n\n#Modeling\nfrom torch.utils.data import DataLoader\nfrom torch.optim import SGD, Adam\nfrom seqeval.metrics import classification_report\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-01-12T10:15:24.545246Z","iopub.execute_input":"2024-01-12T10:15:24.545559Z","iopub.status.idle":"2024-01-12T10:15:24.552049Z","shell.execute_reply.started":"2024-01-12T10:15:24.545532Z","shell.execute_reply":"2024-01-12T10:15:24.551057Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')","metadata":{"execution":{"iopub.status.busy":"2024-01-12T10:15:24.553409Z","iopub.execute_input":"2024-01-12T10:15:24.553770Z","iopub.status.idle":"2024-01-12T10:15:24.863397Z","shell.execute_reply.started":"2024-01-12T10:15:24.553734Z","shell.execute_reply":"2024-01-12T10:15:24.862526Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#hàm đọc dữ liệu\ndef read_dataset(file_path):\n    tokens=[]\n    ner_tags=[]\n    ids=[]\n    count=1\n    with open(file_path) as f:\n        lines=f.readlines()\n        ts=[]\n        nts=[]\n        for line in lines:\n            line = line.split()\n            if len(line)==0:\n                ids.append(count)\n                tokens.append(ts)\n                ner_tags.append(nts)\n                ts=[]\n                nts=[]\n                count+=1\n            else:\n                ts.append(line[0])\n                nts.append(line[-1])\n    data = pd.DataFrame({'Id':ids, 'NER_tags':ner_tags, 'Tokens':tokens})\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-01-12T10:15:24.865667Z","iopub.execute_input":"2024-01-12T10:15:24.865985Z","iopub.status.idle":"2024-01-12T10:15:24.910876Z","shell.execute_reply.started":"2024-01-12T10:15:24.865958Z","shell.execute_reply":"2024-01-12T10:15:24.909670Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def get_NER_labels(data):\n    NERs = list(data['NER_tags'].values)\n    labels_list = []\n    for value in NERs:\n        labels_list = labels_list + value\n    types = list(set(labels_list))\n    return types","metadata":{"execution":{"iopub.status.busy":"2024-01-12T10:15:24.912101Z","iopub.execute_input":"2024-01-12T10:15:24.912464Z","iopub.status.idle":"2024-01-12T10:15:24.926666Z","shell.execute_reply.started":"2024-01-12T10:15:24.912429Z","shell.execute_reply":"2024-01-12T10:15:24.925749Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"train_df = read_dataset('/kaggle/input/covid19vi/syllable/train_syllable.conll')\nval_df = read_dataset('/kaggle/input/covid19vi/syllable/dev_syllable.conll')","metadata":{"execution":{"iopub.status.busy":"2024-01-12T10:15:24.927887Z","iopub.execute_input":"2024-01-12T10:15:24.928202Z","iopub.status.idle":"2024-01-12T10:15:25.188923Z","shell.execute_reply.started":"2024-01-12T10:15:24.928164Z","shell.execute_reply":"2024-01-12T10:15:25.188067Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-12T10:15:25.190256Z","iopub.execute_input":"2024-01-12T10:15:25.190635Z","iopub.status.idle":"2024-01-12T10:15:25.209381Z","shell.execute_reply.started":"2024-01-12T10:15:25.190599Z","shell.execute_reply":"2024-01-12T10:15:25.208498Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"   Id                                           NER_tags  \\\n0   1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n1   2  [O, O, O, O, O, O, O, O, O, O, O, B-SYMPTOM_AN...   \n2   3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n3   4  [O, O, O, O, O, O, O, B-LOCATION, O, B-LOCATIO...   \n4   5  [O, O, O, B-PATIENT_ID, O, O, O, O, O, O, O, B...   \n\n                                              Tokens  \n0  [Đồng, thời, ,, bệnh, viện, tiếp, tục, thực, h...  \n1  [\", Số, bệnh, viện, có, thể, tiếp, nhận, bệnh,...  \n2  [Ngoài, ra, ,, những, người, tiếp, xúc, gián, ...  \n3  [Bà, này, khi, trở, về, quá, cảnh, Doha, (, Qa...  \n4  [\", Bệnh, nhân, 523, \", và, chồng, là, \", bệnh...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>NER_tags</th>\n      <th>Tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[Đồng, thời, ,, bệnh, viện, tiếp, tục, thực, h...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, B-SYMPTOM_AN...</td>\n      <td>[\", Số, bệnh, viện, có, thể, tiếp, nhận, bệnh,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[Ngoài, ra, ,, những, người, tiếp, xúc, gián, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[O, O, O, O, O, O, O, B-LOCATION, O, B-LOCATIO...</td>\n      <td>[Bà, này, khi, trở, về, quá, cảnh, Doha, (, Qa...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[O, O, O, B-PATIENT_ID, O, O, O, O, O, O, O, B...</td>\n      <td>[\", Bệnh, nhân, 523, \", và, chồng, là, \", bệnh...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"unique_labels = get_NER_labels(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T10:15:25.210585Z","iopub.execute_input":"2024-01-12T10:15:25.210935Z","iopub.status.idle":"2024-01-12T10:15:27.519966Z","shell.execute_reply.started":"2024-01-12T10:15:25.210896Z","shell.execute_reply":"2024-01-12T10:15:27.519148Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"unique_labels","metadata":{"execution":{"iopub.status.busy":"2024-01-12T10:15:27.521233Z","iopub.execute_input":"2024-01-12T10:15:27.521539Z","iopub.status.idle":"2024-01-12T10:15:27.528278Z","shell.execute_reply.started":"2024-01-12T10:15:27.521513Z","shell.execute_reply":"2024-01-12T10:15:27.527256Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"['I-PATIENT_ID',\n 'B-AGE',\n 'I-SYMPTOM_AND_DISEASE',\n 'B-LOCATION',\n 'O',\n 'I-NAME',\n 'B-PATIENT_ID',\n 'I-LOCATION',\n 'I-TRANSPORTATION',\n 'I-JOB',\n 'B-DATE',\n 'I-GENDER',\n 'B-JOB',\n 'B-GENDER',\n 'B-TRANSPORTATION',\n 'I-AGE',\n 'B-ORGANIZATION',\n 'I-DATE',\n 'I-ORGANIZATION',\n 'B-SYMPTOM_AND_DISEASE',\n 'B-NAME']"},"metadata":{}}]},{"cell_type":"markdown","source":"# **TẠO LỚP DATASET BẰNG PYTORCH DATASET**","metadata":{}},{"cell_type":"code","source":"def align_label(text, labels, flag=False):\n    label_all_tokens = flag #flag xác định cách thực hiện align_label\n    \n    tokenized_input = tokenizer(text, padding='max_length', max_length=512, truncation=True, is_split_into_words=True)\n\n    word_ids = tokenized_input.word_ids()\n\n    previous_word_idx = None\n    label_ids = []\n\n    for word_idx in word_ids:\n\n        if word_idx is None:\n            label_ids.append(-100)\n\n        elif word_idx != previous_word_idx:\n            try:\n                label_ids.append(labels_to_ids[labels[word_idx]])\n            except:\n                label_ids.append(-100)\n        else:\n            try:\n                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n            except:\n                label_ids.append(-100)\n        previous_word_idx = word_idx\n\n    return label_ids\n\nclass DataSet(torch.utils.data.Dataset):\n\n    def __init__(self, df, flag_align_label=False):\n\n        lb = df['NER_tags'].values.tolist()\n        txt = df['Tokens'].values.tolist()\n        self.texts = [tokenizer(i, padding='max_length', max_length = 512,\n                                truncation=True, return_tensors=\"pt\", is_split_into_words=True) for i in txt]\n        self.labels = [align_label(i,j,flag_align_label) for i,j in zip(txt, lb)]\n\n    def __len__(self):\n\n        return len(self.labels)\n\n    def get_data(self, idx):\n        return self.texts[idx]\n\n    def get_labels(self, idx):\n        return torch.LongTensor(self.labels[idx])\n\n    def __getitem__(self, idx):\n        data = self.get_data(idx)\n        labels = self.get_labels(idx)\n\n        return data, labels","metadata":{"execution":{"iopub.status.busy":"2024-01-12T10:15:27.532874Z","iopub.execute_input":"2024-01-12T10:15:27.533175Z","iopub.status.idle":"2024-01-12T10:15:27.546020Z","shell.execute_reply.started":"2024-01-12T10:15:27.533145Z","shell.execute_reply":"2024-01-12T10:15:27.545069Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"**Tạo 2 dictionary để xác định id nào sẽ là label nào và ngược lại.**","metadata":{}},{"cell_type":"code","source":"labels_to_ids = {k: v for v, k in enumerate(unique_labels)}\nids_to_labels = {v: k for v, k in enumerate(unique_labels)}","metadata":{"execution":{"iopub.status.busy":"2024-01-12T10:15:27.547108Z","iopub.execute_input":"2024-01-12T10:15:27.547437Z","iopub.status.idle":"2024-01-12T10:15:27.563068Z","shell.execute_reply.started":"2024-01-12T10:15:27.547411Z","shell.execute_reply":"2024-01-12T10:15:27.562348Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"labels_to_ids","metadata":{"execution":{"iopub.status.busy":"2024-01-12T10:15:27.563954Z","iopub.execute_input":"2024-01-12T10:15:27.564243Z","iopub.status.idle":"2024-01-12T10:15:27.576724Z","shell.execute_reply.started":"2024-01-12T10:15:27.564217Z","shell.execute_reply":"2024-01-12T10:15:27.575904Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"{'I-PATIENT_ID': 0,\n 'B-AGE': 1,\n 'I-SYMPTOM_AND_DISEASE': 2,\n 'B-LOCATION': 3,\n 'O': 4,\n 'I-NAME': 5,\n 'B-PATIENT_ID': 6,\n 'I-LOCATION': 7,\n 'I-TRANSPORTATION': 8,\n 'I-JOB': 9,\n 'B-DATE': 10,\n 'I-GENDER': 11,\n 'B-JOB': 12,\n 'B-GENDER': 13,\n 'B-TRANSPORTATION': 14,\n 'I-AGE': 15,\n 'B-ORGANIZATION': 16,\n 'I-DATE': 17,\n 'I-ORGANIZATION': 18,\n 'B-SYMPTOM_AND_DISEASE': 19,\n 'B-NAME': 20}"},"metadata":{}}]},{"cell_type":"markdown","source":"# **BUILD MODEL BERT FOR TOKEN CLASSIFICATION**","metadata":{}},{"cell_type":"code","source":"class BertModel(torch.nn.Module):\n\n    def __init__(self):\n\n        super(BertModel, self).__init__()\n\n        self.bert = BertForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(unique_labels))\n\n    def forward(self, input_id, mask, label):\n\n        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-01-12T10:15:27.577767Z","iopub.execute_input":"2024-01-12T10:15:27.578055Z","iopub.status.idle":"2024-01-12T10:15:27.586883Z","shell.execute_reply.started":"2024-01-12T10:15:27.578031Z","shell.execute_reply":"2024-01-12T10:15:27.585922Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"# **TRAIN MODEL**","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-01-12T10:15:27.587943Z","iopub.execute_input":"2024-01-12T10:15:27.588236Z","iopub.status.idle":"2024-01-12T10:15:27.599545Z","shell.execute_reply.started":"2024-01-12T10:15:27.588212Z","shell.execute_reply":"2024-01-12T10:15:27.598831Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"def train_loop(model, train_df, val_df, flag_align_label):\n\n    train_dataset = DataSet(train_df, flag_align_label)\n    val_dataset = DataSet(val_df, flag_align_label)\n\n    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n    val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n\n    if use_cuda:\n        model = model.cuda()\n        \n    min_val_loss = 1000\n    count = 0\n    \n    for epoch_num in range(EPOCHS):\n\n        total_acc_train = 0\n        total_loss_train = 0\n\n        model.train()\n\n        for train_data, train_label in tqdm(train_dataloader):\n\n            train_label = train_label.to(device)\n            mask = train_data['attention_mask'].squeeze(1).to(device)\n            input_id = train_data['input_ids'].squeeze(1).to(device)\n\n            optimizer.zero_grad()\n            loss, logits = model(input_id, mask, train_label)\n\n            for i in range(logits.shape[0]):\n\n                logits_clean = logits[i][train_label[i] != -100]\n                label_clean = train_label[i][train_label[i] != -100]\n\n                predictions = logits_clean.argmax(dim=1)\n                acc = (predictions == label_clean).float().mean()\n                total_acc_train += acc\n                total_loss_train += loss.item()\n\n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n\n        total_acc_val = 0\n        total_loss_val = 0\n\n        for val_data, val_label in val_dataloader:\n\n            val_label = val_label.to(device)\n            mask = val_data['attention_mask'].squeeze(1).to(device)\n            input_id = val_data['input_ids'].squeeze(1).to(device)\n\n            loss, logits = model(input_id, mask, val_label)\n\n            for i in range(logits.shape[0]):\n\n                logits_clean = logits[i][val_label[i] != -100]\n                label_clean = val_label[i][val_label[i] != -100]\n\n                predictions = logits_clean.argmax(dim=1)\n                acc = (predictions == label_clean).float().mean()\n                total_acc_val += acc\n                total_loss_val += loss.item()\n\n        val_accuracy = total_acc_val / len(val_df)\n        val_loss = total_loss_val / len(val_df)\n\n        print(\n            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(train_df): .3f} | Accuracy: {total_acc_train / len(train_df): .3f} | Val_Loss: {total_loss_val / len(val_df): .3f} | Accuracy: {total_acc_val / len(val_df): .3f}')\n        if val_loss < min_val_loss:\n            min_val_loss = val_loss\n            torch.save(model.state_dict(), 'mbert_ner')\n            count = epoch_num\n        if epoch_num - count >= 5:\n            return\n        \nLEARNING_RATE = 5e-5\nEPOCHS = 30\nBATCH_SIZE = 8\n\nmodel = BertModel()\ntrain_loop(model, train_df, val_df, False)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T10:15:27.600642Z","iopub.execute_input":"2024-01-12T10:15:27.600954Z","iopub.status.idle":"2024-01-12T11:07:37.785542Z","shell.execute_reply.started":"2024-01-12T10:15:27.600929Z","shell.execute_reply":"2024-01-12T11:07:37.784318Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|██████████| 629/629 [04:36<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 1 | Loss:  0.153 | Accuracy:  0.961 | Val_Loss:  0.096 | Accuracy:  0.974\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 629/629 [04:36<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 2 | Loss:  0.063 | Accuracy:  0.983 | Val_Loss:  0.104 | Accuracy:  0.976\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 629/629 [04:36<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 3 | Loss:  0.043 | Accuracy:  0.989 | Val_Loss:  0.102 | Accuracy:  0.978\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 629/629 [04:36<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 4 | Loss:  0.039 | Accuracy:  0.990 | Val_Loss:  0.109 | Accuracy:  0.976\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 629/629 [04:36<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 5 | Loss:  0.030 | Accuracy:  0.993 | Val_Loss:  0.089 | Accuracy:  0.979\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 629/629 [04:36<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 6 | Loss:  0.028 | Accuracy:  0.993 | Val_Loss:  0.099 | Accuracy:  0.980\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 629/629 [04:36<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 7 | Loss:  0.028 | Accuracy:  0.993 | Val_Loss:  0.109 | Accuracy:  0.976\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 629/629 [04:36<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 8 | Loss:  0.026 | Accuracy:  0.993 | Val_Loss:  0.102 | Accuracy:  0.979\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 629/629 [04:36<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 9 | Loss:  0.022 | Accuracy:  0.994 | Val_Loss:  0.107 | Accuracy:  0.978\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 629/629 [04:36<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 10 | Loss:  0.017 | Accuracy:  0.995 | Val_Loss:  0.108 | Accuracy:  0.978\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **EVALUATE MODEL**","metadata":{}},{"cell_type":"code","source":"test_df = read_dataset('/kaggle/input/covid19vi/syllable/test_syllable.conll')","metadata":{"execution":{"iopub.status.busy":"2024-01-12T11:07:37.787602Z","iopub.execute_input":"2024-01-12T11:07:37.787950Z","iopub.status.idle":"2024-01-12T11:07:37.917801Z","shell.execute_reply.started":"2024-01-12T11:07:37.787919Z","shell.execute_reply":"2024-01-12T11:07:37.916934Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, test_df, flag_align_label, ids_to_labels):\n\n    test_dataset = DataSet(test_df, flag_align_label)\n\n    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    if use_cuda:\n        model = model.cuda()\n\n    predictions = []\n    labels = []\n    \n    for test_data, test_label in test_dataloader:\n\n            test_label = test_label.to(device)\n            mask = test_data['attention_mask'].squeeze(1).to(device)\n\n            input_id = test_data['input_ids'].squeeze(1).to(device)\n\n            loss, logits = model(input_id, mask, test_label)\n\n            for i in range(logits.shape[0]):\n                cleaned_logits = logits[i][test_label[i] != -100].argmax(dim=1)\n                predictions.append([ids_to_labels[val.item()] for val in cleaned_logits])\n                cleaned_labels = test_label[i][test_label[i] != -100]\n                labels.append([ids_to_labels[val.item()] for val in cleaned_labels])\n    print(classification_report(y_pred=predictions, y_true=labels, digits=3))\n\nevaluate(model, test_df, False, ids_to_labels)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T11:07:37.919095Z","iopub.execute_input":"2024-01-12T11:07:37.919450Z","iopub.status.idle":"2024-01-12T11:09:00.132978Z","shell.execute_reply.started":"2024-01-12T11:07:37.919424Z","shell.execute_reply":"2024-01-12T11:09:00.131728Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"                     precision    recall  f1-score   support\n\n                AGE      0.964     0.957     0.960       582\n               DATE      0.978     0.984     0.981      1654\n             GENDER      0.965     0.957     0.961       462\n                JOB      0.671     0.636     0.653       173\n           LOCATION      0.913     0.929     0.921      4441\n               NAME      0.920     0.934     0.927       318\n       ORGANIZATION      0.790     0.844     0.816       771\n         PATIENT_ID      0.968     0.984     0.976      2005\nSYMPTOM_AND_DISEASE      0.813     0.787     0.800      1136\n     TRANSPORTATION      0.934     0.959     0.946       193\n\n          micro avg      0.915     0.926     0.920     11735\n          macro avg      0.892     0.897     0.894     11735\n       weighted avg      0.915     0.926     0.920     11735\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **INFERENCE**","metadata":{}},{"cell_type":"code","source":"def align_word_ids(text, flag):\n    label_all_tokens = flag\n    \n    text = text.split()\n  \n    tokenized_inputs = tokenizer(text, padding='max_length', max_length=512, truncation=True, is_split_into_words=True)\n\n    word_ids = tokenized_inputs.word_ids()\n\n    previous_word_idx = None\n    label_ids = []\n\n    for word_idx in word_ids:\n\n        if word_idx is None:\n            label_ids.append(-100)\n\n        elif word_idx != previous_word_idx:\n            try:\n                label_ids.append(1)\n            except:\n                label_ids.append(-100)\n        else:\n            try:\n                label_ids.append(1 if label_all_tokens else -100)\n            except:\n                label_ids.append(-100)\n        previous_word_idx = word_idx\n\n    return label_ids","metadata":{"execution":{"iopub.status.busy":"2024-01-12T11:09:00.135091Z","iopub.execute_input":"2024-01-12T11:09:00.135415Z","iopub.status.idle":"2024-01-12T11:09:00.143627Z","shell.execute_reply.started":"2024-01-12T11:09:00.135386Z","shell.execute_reply":"2024-01-12T11:09:00.142608Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def ner(model, sentence, flag_align_label):\n\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    if use_cuda:\n        model = model.cuda()\n\n    text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n\n    mask = text['attention_mask'].to(device)\n    input_id = text['input_ids'].to(device)\n    label_ids = torch.Tensor(align_word_ids(sentence, flag_align_label)).unsqueeze(0).to(device)\n\n    logits = model(input_id, mask, None)\n    logits_clean = logits[0][label_ids != -100]\n\n    predictions = logits_clean.argmax(dim=1).tolist()\n    prediction_label = [ids_to_labels[i] for i in predictions]\n    print(sentence)\n    print(prediction_label)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T11:09:00.144953Z","iopub.execute_input":"2024-01-12T11:09:00.145273Z","iopub.status.idle":"2024-01-12T11:09:00.156856Z","shell.execute_reply.started":"2024-01-12T11:09:00.145234Z","shell.execute_reply":"2024-01-12T11:09:00.155931Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"ner(model,\n    'Bệnh nhân nhập viện tối qua ở Bệnh Viện 115 là bệnh nhân thứ 82 , di chuyển qua nhiều thành phố bằng xe biển hiệu E-402',\n    flag_align_label=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T11:09:00.158422Z","iopub.execute_input":"2024-01-12T11:09:00.159060Z","iopub.status.idle":"2024-01-12T11:09:00.206105Z","shell.execute_reply.started":"2024-01-12T11:09:00.159022Z","shell.execute_reply":"2024-01-12T11:09:00.205048Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Bệnh nhân nhập viện tối qua ở Bệnh Viện 115 là bệnh nhân thứ 82 , di chuyển qua nhiều thành phố bằng xe biển hiệu E-402\n['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'B-PATIENT_ID', 'O', 'O', 'O', 'O', 'B-PATIENT_ID', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TRANSPORTATION']\n","output_type":"stream"}]}]}