{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6895280,"sourceType":"datasetVersion","datasetId":3960915}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#tải các thư viện cần thiết\n!pip install transformer\n!pip install seqeval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-17T17:17:53.566924Z","iopub.execute_input":"2024-01-17T17:17:53.567291Z","iopub.status.idle":"2024-01-17T17:18:12.392336Z","shell.execute_reply.started":"2024-01-17T17:17:53.567260Z","shell.execute_reply":"2024-01-17T17:18:12.391237Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement transformer (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for transformer\u001b[0m\u001b[31m\n\u001b[0mCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.24.3)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=3c6b4098bd7516757e63ac8b28250bda95f0b620f3e694281b24e27f9bce038d\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"}]},{"cell_type":"code","source":"#import các thư viện cần thiết\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch import autograd\n\n#Embedding\nfrom transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification\n\n#Modeling\nfrom torch.utils.data import DataLoader\nfrom torch.optim import SGD, Adam\nfrom seqeval.metrics import classification_report\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:12.394643Z","iopub.execute_input":"2024-01-17T17:18:12.395013Z","iopub.status.idle":"2024-01-17T17:18:18.258504Z","shell.execute_reply.started":"2024-01-17T17:18:12.394977Z","shell.execute_reply":"2024-01-17T17:18:18.257670Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:18.259596Z","iopub.execute_input":"2024-01-17T17:18:18.260035Z","iopub.status.idle":"2024-01-17T17:18:20.562942Z","shell.execute_reply.started":"2024-01-17T17:18:18.260008Z","shell.execute_reply":"2024-01-17T17:18:20.562032Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c7aabd3ed8e45e7a72747f93dad1b60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5260bc1246b14e90b366eb066385f357"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d69d1c460f0d434fba7f67c09d40323d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec0133753b6a4d21a1edc9a1a5048055"}},"metadata":{}}]},{"cell_type":"code","source":"#hàm đọc dữ liệu\ndef read_dataset(file_path):\n    tokens=[]\n    ner_tags=[]\n    ids=[]\n    count=1\n    with open(file_path) as f:\n        lines=f.readlines()\n        ts=[]\n        nts=[]\n        for line in lines:\n            line = line.split()\n            if len(line)==0:\n                ids.append(count)\n                tokens.append(ts)\n                ner_tags.append(nts)\n                ts=[]\n                nts=[]\n                count+=1\n            else:\n                ts.append(line[0])\n                nts.append(line[-1])\n    data = pd.DataFrame({'Id':ids, 'NER_tags':ner_tags, 'Tokens':tokens})\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:20.564878Z","iopub.execute_input":"2024-01-17T17:18:20.565201Z","iopub.status.idle":"2024-01-17T17:18:20.572408Z","shell.execute_reply.started":"2024-01-17T17:18:20.565175Z","shell.execute_reply":"2024-01-17T17:18:20.571524Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_NER_labels(data):\n    NERs = list(data['NER_tags'].values)\n    labels_list = []\n    for value in NERs:\n        labels_list = labels_list + value\n    types = list(set(labels_list))\n    return types","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:20.573601Z","iopub.execute_input":"2024-01-17T17:18:20.573863Z","iopub.status.idle":"2024-01-17T17:18:20.584301Z","shell.execute_reply.started":"2024-01-17T17:18:20.573841Z","shell.execute_reply":"2024-01-17T17:18:20.583618Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_df = read_dataset('/kaggle/input/covid19vi/word/train_word.conll')\nval_df = read_dataset('/kaggle/input/covid19vi/word/dev_word.conll')","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:20.585447Z","iopub.execute_input":"2024-01-17T17:18:20.585804Z","iopub.status.idle":"2024-01-17T17:18:20.834753Z","shell.execute_reply.started":"2024-01-17T17:18:20.585774Z","shell.execute_reply":"2024-01-17T17:18:20.833726Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:20.836057Z","iopub.execute_input":"2024-01-17T17:18:20.837045Z","iopub.status.idle":"2024-01-17T17:18:20.861184Z","shell.execute_reply.started":"2024-01-17T17:18:20.837010Z","shell.execute_reply":"2024-01-17T17:18:20.860302Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   Id                                           NER_tags  \\\n0   1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n1   2  [O, O, O, O, O, O, O, B-SYMPTOM_AND_DISEASE, I...   \n2   3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n3   4  [O, O, O, O, O, O, B-LOCATION, O, B-LOCATION, ...   \n4   5  [O, O, B-PATIENT_ID, O, O, O, O, O, O, B-PATIE...   \n\n                                              Tokens  \n0  [Đồng_thời, ,, bệnh_viện, tiếp_tục, thực_hiện,...  \n1  [\", Số, bệnh_viện, có_thể, tiếp_nhận, bệnh_nhâ...  \n2  [Ngoài_ra, ,, những, người, tiếp_xúc, gián_tiế...  \n3  [Bà, này, khi, trở, về, quá_cảnh, Doha, (, Qat...  \n4  [\", Bệnh_nhân, 523, \", và, chồng, là, \", bệnh_...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>NER_tags</th>\n      <th>Tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[Đồng_thời, ,, bệnh_viện, tiếp_tục, thực_hiện,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[O, O, O, O, O, O, O, B-SYMPTOM_AND_DISEASE, I...</td>\n      <td>[\", Số, bệnh_viện, có_thể, tiếp_nhận, bệnh_nhâ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[Ngoài_ra, ,, những, người, tiếp_xúc, gián_tiế...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[O, O, O, O, O, O, B-LOCATION, O, B-LOCATION, ...</td>\n      <td>[Bà, này, khi, trở, về, quá_cảnh, Doha, (, Qat...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[O, O, B-PATIENT_ID, O, O, O, O, O, O, B-PATIE...</td>\n      <td>[\", Bệnh_nhân, 523, \", và, chồng, là, \", bệnh_...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"unique_labels = get_NER_labels(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:20.862310Z","iopub.execute_input":"2024-01-17T17:18:20.862640Z","iopub.status.idle":"2024-01-17T17:18:22.653497Z","shell.execute_reply.started":"2024-01-17T17:18:20.862612Z","shell.execute_reply":"2024-01-17T17:18:22.652669Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **TẠO LỚP DATASET BẰNG PYTORCH DATASET**","metadata":{}},{"cell_type":"code","source":"def align_label(text, labels, flag=False):\n    label_all_tokens = flag #flag xác định cách thực hiện align_label\n    \n    tokenized_input = tokenizer(text, padding='max_length', max_length=256, truncation=True, is_split_into_words=True)\n    word_ids = tokenized_input.input_ids\n    \n    start_part = True\n    label_ids = []\n    count = 0\n    \n    for i in range(len(word_ids)):\n        \n        if word_ids[i] == 0 or word_ids[i] == 1 or word_ids[i] == 2:\n            label_ids.append(-100)\n            \n        elif count < len(text) and ''.join(tokenizer.decode(tokenized_input['input_ids'][i]).split()) == text[count]:\n            label_ids.append(labels_to_ids[labels[count]])\n            count+=1\n            start_part = True\n        else:\n            if start_part:\n                label_ids.append(labels_to_ids[labels[count]])\n                count+=1\n                start_part = False\n            else:\n                label_ids.append(labels_to_ids[labels[count]] if label_all_tokens else -100)\n                \n    return label_ids\n\nclass DataSet(torch.utils.data.Dataset):\n\n    def __init__(self, df, flag_align_label=False):\n\n        lb = df['NER_tags'].values.tolist()\n        txt = df['Tokens'].values.tolist()\n        self.texts = [tokenizer(i, padding='max_length', max_length = 256,\n                                truncation=True, return_tensors=\"pt\", is_split_into_words=True) for i in txt]\n        self.labels = [align_label(i,j,flag_align_label) for i,j in zip(txt, lb)]\n        \n    def __len__(self):\n\n        return len(self.labels)\n\n    def get_data(self, idx):\n        return self.texts[idx]\n\n    def get_labels(self, idx):\n        return torch.LongTensor(self.labels[idx])\n\n    def __getitem__(self, idx):\n        data = self.get_data(idx)\n        labels = self.get_labels(idx)\n\n        return data, labels","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:22.654774Z","iopub.execute_input":"2024-01-17T17:18:22.655538Z","iopub.status.idle":"2024-01-17T17:18:22.668554Z","shell.execute_reply.started":"2024-01-17T17:18:22.655500Z","shell.execute_reply":"2024-01-17T17:18:22.667547Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Tạo 2 dictionary để xác định id nào sẽ là label nào và ngược lại.**","metadata":{}},{"cell_type":"code","source":"labels_to_ids = {k: (v+1) for v, k in enumerate(unique_labels)}\nids_to_labels = {(v+1): k for v, k in enumerate(unique_labels)}","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:22.672933Z","iopub.execute_input":"2024-01-17T17:18:22.673447Z","iopub.status.idle":"2024-01-17T17:18:22.683305Z","shell.execute_reply.started":"2024-01-17T17:18:22.673421Z","shell.execute_reply":"2024-01-17T17:18:22.682519Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"labels_to_ids['START_TAG']=0\nlabels_to_ids['STOP_TAG']=len(unique_labels)+1\nids_to_labels[0]='START_TAG'\nids_to_labels[len(unique_labels)+1]='STOP_TAG'","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:22.684541Z","iopub.execute_input":"2024-01-17T17:18:22.685106Z","iopub.status.idle":"2024-01-17T17:18:22.692848Z","shell.execute_reply.started":"2024-01-17T17:18:22.685053Z","shell.execute_reply":"2024-01-17T17:18:22.692115Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"labels_to_ids","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:22.693820Z","iopub.execute_input":"2024-01-17T17:18:22.694058Z","iopub.status.idle":"2024-01-17T17:18:22.708887Z","shell.execute_reply.started":"2024-01-17T17:18:22.694037Z","shell.execute_reply":"2024-01-17T17:18:22.707973Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'I-SYMPTOM_AND_DISEASE': 1,\n 'I-JOB': 2,\n 'B-SYMPTOM_AND_DISEASE': 3,\n 'I-LOCATION': 4,\n 'I-TRANSPORTATION': 5,\n 'B-DATE': 6,\n 'B-JOB': 7,\n 'I-PATIENT_ID': 8,\n 'I-DATE': 9,\n 'I-NAME': 10,\n 'B-ORGANIZATION': 11,\n 'B-TRANSPORTATION': 12,\n 'B-NAME': 13,\n 'I-ORGANIZATION': 14,\n 'B-AGE': 15,\n 'I-AGE': 16,\n 'O': 17,\n 'B-LOCATION': 18,\n 'B-PATIENT_ID': 19,\n 'B-GENDER': 20,\n 'START_TAG': 0,\n 'STOP_TAG': 21}"},"metadata":{}}]},{"cell_type":"markdown","source":"# **CÁC HÀM CHO BỘ PHÂN LOẠI CRF**","metadata":{}},{"cell_type":"code","source":"def log_sum_exp(vec):\n    '''\n    This function calculates the score explained above for the forward algorithm\n    vec 2D: 1 * tagset_size\n    '''\n    max_score = vec[0, argmax(vec)]\n    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n    \ndef argmax(vec):\n    '''\n    This function returns the max index in a vector\n    '''\n    _, idx = torch.max(vec, 1)\n    return to_scalar(idx)\n\ndef to_scalar(var):\n    '''\n    Function to convert pytorch tensor to a scalar\n    '''\n    return var.view(-1).data.tolist()[0]","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:22.709998Z","iopub.execute_input":"2024-01-17T17:18:22.710347Z","iopub.status.idle":"2024-01-17T17:18:22.719396Z","shell.execute_reply.started":"2024-01-17T17:18:22.710323Z","shell.execute_reply":"2024-01-17T17:18:22.718661Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def score_sentences(self, feats, tags):\n    r = torch.LongTensor(range(feats.size()[0]))\n    if self.use_gpu:\n        r = r.cuda()\n        pad_start_tags = torch.cat([torch.cuda.LongTensor([self.tag_to_ix['START_TAG']]), tags])\n        pad_stop_tags = torch.cat([tags, torch.cuda.LongTensor([self.tag_to_ix['STOP_TAG']])])\n    else:\n        pad_start_tags = torch.cat([torch.LongTensor([self.tag_to_ix['START_TAG']]), tags])\n        pad_stop_tags = torch.cat([tags, torch.LongTensor([self.tag_to_ix['STOP_TAG']])])\n\n    score = torch.sum(self.transitions[pad_stop_tags, pad_start_tags]) + torch.sum(feats[r, tags])\n\n    return score","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:22.720427Z","iopub.execute_input":"2024-01-17T17:18:22.720755Z","iopub.status.idle":"2024-01-17T17:18:22.729893Z","shell.execute_reply.started":"2024-01-17T17:18:22.720731Z","shell.execute_reply":"2024-01-17T17:18:22.728990Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def forward_alg(self, feats):\n    '''\n    This function performs the forward algorithm\n    '''\n    # calculate in log domain\n    # feats is len(sentence) * tagset_size\n    # initialize alpha with a Tensor with values all equal to -10000.\n    \n    # Do the forward algorithm to compute the partition function\n    init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n    \n    # START_TAG has all of the score.\n    init_alphas[0][self.tag_to_ix['START_TAG']] = 0.\n    \n    # Wrap in a variable so that we will get automatic backprop\n    forward_var = autograd.Variable(init_alphas)\n    if self.use_gpu:\n        forward_var = forward_var.cuda()\n        \n    # Iterate through the sentence\n    for feat in feats:\n        # broadcast the emission score: it is the same regardless of\n        # the previous tag\n        emit_score = feat.view(-1, 1)\n        \n        # the ith entry of trans_score is the score of transitioning to\n        # next_tag from i\n        tag_var = forward_var + self.transitions + emit_score\n        \n        # The ith entry of next_tag_var is the value for the\n        # edge (i -> next_tag) before we do log-sum-exp\n        max_tag_var, _ = torch.max(tag_var, dim=1)\n        \n        # The forward variable for this tag is log-sum-exp of all the\n        # scores.\n        tag_var = tag_var - max_tag_var.view(-1, 1)\n        \n        # Compute log sum exp in a numerically stable way for the forward algorithm\n        forward_var = max_tag_var + torch.log(torch.sum(torch.exp(tag_var), dim=1)).view(1, -1) # ).view(1, -1)\n    terminal_var = (forward_var + self.transitions[self.tag_to_ix['STOP_TAG']]).view(1, -1)\n    alpha = log_sum_exp(terminal_var)\n    # Z(x)\n    return alpha","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:22.731147Z","iopub.execute_input":"2024-01-17T17:18:22.731832Z","iopub.status.idle":"2024-01-17T17:18:22.741589Z","shell.execute_reply.started":"2024-01-17T17:18:22.731806Z","shell.execute_reply":"2024-01-17T17:18:22.740749Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def get_neg_log_likelihood(self, feats, tags):\n    # sentence, tags is a list of ints\n    # features is a 2D tensor, len(sentence) * self.tagset_size\n\n    forward_score = self._forward_alg(feats)\n    gold_score = self._score_sentence(feats, tags)\n    return forward_score - gold_score","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:22.742674Z","iopub.execute_input":"2024-01-17T17:18:22.742947Z","iopub.status.idle":"2024-01-17T17:18:22.757887Z","shell.execute_reply.started":"2024-01-17T17:18:22.742924Z","shell.execute_reply":"2024-01-17T17:18:22.757051Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# **BUILD MODEL PHOBERT FOR TOKEN CLASSIFICATION**","metadata":{}},{"cell_type":"code","source":"class PhoBertModel(torch.nn.Module):\n\n    def __init__(self):\n\n        super(PhoBertModel, self).__init__()\n        \n        self.tag_to_ix = labels_to_ids\n        self.use_gpu = True\n        self.tagset_size = len(labels_to_ids)\n        self.transitions = nn.Parameter(torch.zeros(self.tagset_size, self.tagset_size))\n        self.transitions.data[labels_to_ids['START_TAG'], :] = -10000\n        self.transitions.data[:, labels_to_ids['STOP_TAG']] = -10000\n        self.phobert = AutoModelForTokenClassification.from_pretrained(\"vinai/phobert-base\", num_labels=(len(unique_labels)+2))\n\n    def forward(self, input_id, mask, label):\n        \n        output = self.phobert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n        \n        return output\n    neg_log_likelihood = get_neg_log_likelihood\n    _forward_alg = forward_alg\n    _score_sentence = score_sentences","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:22.759585Z","iopub.execute_input":"2024-01-17T17:18:22.759883Z","iopub.status.idle":"2024-01-17T17:18:22.769830Z","shell.execute_reply.started":"2024-01-17T17:18:22.759859Z","shell.execute_reply":"2024-01-17T17:18:22.768988Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# **TRAIN MODEL**","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:22.770870Z","iopub.execute_input":"2024-01-17T17:18:22.771218Z","iopub.status.idle":"2024-01-17T17:18:22.783119Z","shell.execute_reply.started":"2024-01-17T17:18:22.771193Z","shell.execute_reply":"2024-01-17T17:18:22.782274Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def train_loop(model, train_df, val_df, flag_align_label):\n\n    train_dataset = DataSet(train_df, flag_align_label)\n    val_dataset = DataSet(val_df, flag_align_label)\n\n    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n    val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n\n    if use_cuda:\n        model = model.cuda()\n        \n    min_val_loss = 1000\n    count = 0\n    \n    for epoch_num in range(EPOCHS):\n\n        total_loss_train = 0\n\n        model.train()\n\n        for train_data, train_label in tqdm(train_dataloader):\n\n            train_label = train_label.to(device)\n            mask = train_data['attention_mask'].squeeze(1).to(device)\n            input_id = train_data['input_ids'].squeeze(1).to(device)\n\n            optimizer.zero_grad()\n            _, logits = model(input_id, mask, train_label)\n\n\n            logits_clean = logits[0][train_label[0] != -100]\n            label_clean = train_label[0][train_label[0] != -100]\n\n            loss=model.neg_log_likelihood(logits_clean, label_clean)\n            total_loss_train += loss.data\n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n\n        total_loss_val = 0\n\n        for val_data, val_label in val_dataloader:\n\n            val_label = val_label.to(device)\n            mask = val_data['attention_mask'].squeeze(1).to(device)\n            input_id = val_data['input_ids'].squeeze(1).to(device)\n\n            _, logits = model(input_id, mask, val_label)\n\n            logits_clean = logits[0][val_label[0] != -100]\n            label_clean = val_label[0][val_label[0] != -100]\n            loss=model.neg_log_likelihood(logits_clean, label_clean)\n            total_loss_val += loss.data\n\n\n        print(\n            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(train_df): .3f} | Val_Loss: {total_loss_val / len(val_df): .3f}')\n        if (total_loss_val / len(val_df)) < min_val_loss:\n            min_val_loss = total_loss_val / len(val_df)\n            torch.save(model.state_dict(), 'phobert_base_crf_ner')\n            count = epoch_num\n        if epoch_num - count >= 5:\n            return\n        \nLEARNING_RATE = 5e-5\nEPOCHS = 30\nBATCH_SIZE = 1\n\nmodel = PhoBertModel()\ntrain_loop(model, train_df, val_df, False)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T17:18:22.784230Z","iopub.execute_input":"2024-01-17T17:18:22.784533Z","iopub.status.idle":"2024-01-17T18:23:27.496747Z","shell.execute_reply.started":"2024-01-17T17:18:22.784503Z","shell.execute_reply":"2024-01-17T18:23:27.495489Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4156d4c3287463482b94c311197b673"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|██████████| 5027/5027 [05:44<00:00, 14.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 1 | Loss:  3.810 | Val_Loss:  2.194\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5027/5027 [05:44<00:00, 14.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 2 | Loss:  1.455 | Val_Loss:  2.393\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5027/5027 [05:44<00:00, 14.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 3 | Loss:  1.310 | Val_Loss:  2.681\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5027/5027 [05:43<00:00, 14.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 4 | Loss:  1.745 | Val_Loss:  3.706\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5027/5027 [05:41<00:00, 14.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 5 | Loss:  1.322 | Val_Loss:  2.158\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5027/5027 [05:41<00:00, 14.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 6 | Loss:  1.128 | Val_Loss:  2.546\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5027/5027 [05:41<00:00, 14.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 7 | Loss:  1.650 | Val_Loss:  2.561\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5027/5027 [05:41<00:00, 14.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 8 | Loss:  1.048 | Val_Loss:  2.414\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5027/5027 [05:41<00:00, 14.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 9 | Loss:  0.682 | Val_Loss:  3.192\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5027/5027 [05:41<00:00, 14.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 10 | Loss:  0.661 | Val_Loss:  2.736\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **EVALUATE MODEL**","metadata":{}},{"cell_type":"code","source":"test_df = read_dataset('/kaggle/input/covid19vi/word/test_word.conll')","metadata":{"execution":{"iopub.status.busy":"2024-01-17T18:23:27.499631Z","iopub.execute_input":"2024-01-17T18:23:27.500692Z","iopub.status.idle":"2024-01-17T18:23:27.619953Z","shell.execute_reply.started":"2024-01-17T18:23:27.500653Z","shell.execute_reply":"2024-01-17T18:23:27.619061Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, test_df, flag_align_label, ids_to_labels):\n\n    test_dataset = DataSet(test_df, flag_align_label)\n\n    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    if use_cuda:\n        model = model.cuda()\n\n    predictions = []\n    labels = []\n    \n    for test_data, test_label in test_dataloader:\n\n            test_label = test_label.to(device)\n            mask = test_data['attention_mask'].squeeze(1).to(device)\n\n            input_id = test_data['input_ids'].squeeze(1).to(device)\n\n            loss, logits = model(input_id, mask, test_label)\n\n            for i in range(logits.shape[0]):\n                cleaned_logits = logits[i][test_label[i] != -100].argmax(dim=1)\n                predictions.append([ids_to_labels[val.item()] for val in cleaned_logits])\n                cleaned_labels = test_label[i][test_label[i] != -100]\n                labels.append([ids_to_labels[val.item()] for val in cleaned_labels])\n    print(classification_report(y_pred=predictions, y_true=labels, digits=3))\n\nevaluate(model, test_df, False, ids_to_labels)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T18:23:27.621083Z","iopub.execute_input":"2024-01-17T18:23:27.621383Z","iopub.status.idle":"2024-01-17T18:24:30.893692Z","shell.execute_reply.started":"2024-01-17T18:23:27.621358Z","shell.execute_reply":"2024-01-17T18:24:30.892574Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"                     precision    recall  f1-score   support\n\n                AGE      0.965     0.981     0.973       568\n               DATE      0.964     0.989     0.977      1641\n             GENDER      0.943     0.978     0.960       458\n                JOB      0.725     0.657     0.689       169\n           LOCATION      0.924     0.951     0.938      4346\n               NAME      0.940     0.895     0.917       314\n       ORGANIZATION      0.806     0.854     0.829       768\n         PATIENT_ID      0.970     0.984     0.977      1982\nSYMPTOM_AND_DISEASE      0.803     0.862     0.831      1135\n     TRANSPORTATION      0.963     0.974     0.968       189\n\n          micro avg      0.918     0.944     0.931     11570\n          macro avg      0.900     0.912     0.906     11570\n       weighted avg      0.919     0.944     0.931     11570\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **INFERENCE**","metadata":{}},{"cell_type":"code","source":"def align_word_ids(text, flag):\n    label_all_tokens = flag\n    \n    text = text.split()\n  \n    tokenized_inputs = tokenizer(text, padding='max_length', max_length=256, truncation=True, is_split_into_words=True)\n\n    word_ids = tokenized_inputs.input_ids\n\n    start_part = True\n    label_ids = []\n    count = 0\n    \n    for i in range(len(word_ids)):\n        \n        if word_ids[i] == 0 or word_ids[i] == 1 or word_ids[i] == 2:\n            label_ids.append(-100)\n            \n        elif count < len(text) and ''.join(tokenizer.decode(tokenized_inputs['input_ids'][i]).split()) == text[count]:\n            label_ids.append(1)\n            count+=1\n            start_part = True\n        else:\n            if start_part:\n                label_ids.append(1)\n                count+=1\n                start_part = False\n            else:\n                label_ids.append(1 if label_all_tokens else -100)           \n    return label_ids","metadata":{"execution":{"iopub.status.busy":"2024-01-17T18:24:30.896963Z","iopub.execute_input":"2024-01-17T18:24:30.897324Z","iopub.status.idle":"2024-01-17T18:24:30.906580Z","shell.execute_reply.started":"2024-01-17T18:24:30.897292Z","shell.execute_reply":"2024-01-17T18:24:30.905558Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def ner(model, sentence, flag_align_label):\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n    if use_cuda:\n        model = model.cuda()\n    text = tokenizer(sentence, padding='max_length', max_length = 256, truncation=True, return_tensors=\"pt\")\n\n    mask = text['attention_mask'].to(device)\n    input_id = text['input_ids'].to(device)\n    label_ids = torch.Tensor(align_word_ids(sentence, flag_align_label)).unsqueeze(0).to(device)\n\n    logits = model(input_id, mask, None)\n    logits_clean = logits[0][label_ids != -100]\n\n    predictions = logits_clean.argmax(dim=1).tolist()\n    prediction_label = [ids_to_labels[i] for i in predictions]\n    print(sentence)\n    print(prediction_label)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T18:24:30.907749Z","iopub.execute_input":"2024-01-17T18:24:30.908035Z","iopub.status.idle":"2024-01-17T18:24:30.923384Z","shell.execute_reply.started":"2024-01-17T18:24:30.908011Z","shell.execute_reply":"2024-01-17T18:24:30.922502Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"ner(model,\n    'Bệnh nhân nhập viện tối qua ở Bệnh Viện 115 là bệnh nhân thứ 82 , di chuyển qua nhiều thành phố bằng xe biển hiệu E-402',\n    flag_align_label=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T18:29:11.102749Z","iopub.execute_input":"2024-01-17T18:29:11.103251Z","iopub.status.idle":"2024-01-17T18:29:11.135448Z","shell.execute_reply.started":"2024-01-17T18:29:11.103215Z","shell.execute_reply":"2024-01-17T18:29:11.134516Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Bệnh nhân nhập viện tối qua ở Bệnh Viện 115 là bệnh nhân thứ 82 , di chuyển qua nhiều thành phố bằng xe biển hiệu E-402\n['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'O', 'O', 'O', 'O', 'B-PATIENT_ID', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TRANSPORTATION']\n","output_type":"stream"}]}]}